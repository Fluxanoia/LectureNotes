\documentclass[a4paper, 12pt, twoside]{article}
\usepackage[left = 3cm, right = 3cm]{geometry}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multicol}

\makeatletter
\newcommand{\Spvek}[2][r]{%
  \gdef\@VORNE{1}
  \left(\hskip-\arraycolsep%
    \begin{array}{#1}\vekSp@lten{#2}\end{array}%
  \hskip-\arraycolsep\right)}

\def\vekSp@lten#1{\xvekSp@lten#1;vekL@stLine;}
\def\vekL@stLine{vekL@stLine}
\def\xvekSp@lten#1;{\def\temp{#1}%
  \ifx\temp\vekL@stLine
  \else
    \ifnum\@VORNE=1\gdef\@VORNE{0}
    \else\@arraycr\fi%
    #1%
    \expandafter\xvekSp@lten
  \fi}
\makeatother

\DeclareMathOperator{\Ker}{Ker}
\DeclareMathOperator{\Ima}{Im}
\DeclareMathOperator{\Spa}{span}

\DeclareMathOperator{\Rank}{rank}
\DeclareMathOperator{\Null}{nullity}

\DeclareMathOperator{\Tr}{trace}

\DeclareMathOperator{\Diag}{diag}

\DeclareMathOperator{\Spec}{spec}

\begin{document}

\title{Linear Algebra 1 (TB2) Notes}
\date{}
\author{\textit{paraphrased by} Tyler Wright}
\maketitle

\vfill

\textit{An important note, these notes are absolutely \textbf{NOT}
  guaranteed to be correct, representative of the course, or rigorous.
  Any result of this is not the author's fault.}

\newpage

\section{Vector Spaces, Fields, and Maps}

\subsection{Groups}

A group is a \textit{non-empty} set ($G$) paired with a
\textit{binary group operation} ($*$) denoted by $(G, *)$.
The following properties hold for all groups (let $(G, *)$
be a group with elements $f, g, h$):

\begin{itemize}
  \item \textbf{Associativity}: $f * (g * h) = (f * g) * h$
  \item \textbf{Identity}: $\exists \, e \in G : e * f = f * e = f$
  \item \textbf{Inverse}: $\exists \, x \in G : x * f = f * x = e$.
\end{itemize}

\textit{A note, for a group $(G, *)$ with $g * h = h * g$ for all $g, h \in G$,
  this group is called \textbf{commutative} or \textbf{abelian}. However, it
  should be emphasised that this is \textbf{not} a necessary condition for
  a group.}

\subsection{The Invertibility of Matrices}

For a matrix $A \in M_{m, n}( \mathbb{F} )$, the following are all
\textbf{equivalent} statements:

\begin{itemize}
  \item $A$ is \textbf{invertible}
  \item $\det{A} \neq 0$
  \item The \textbf{rows} of $A$ are \textbf{linearly independent}
  \item The \textbf{columns} of $A$ are \textbf{linearly independent}
  \item The \textbf{reduced row echelon form} of $A$ is the \textbf{identity}
  \item For all $\textbf{b} \in \mathbb{F}^n, A\textbf{x} = \textbf{b}$ has
        a \textbf{unique solution}.
\end{itemize}

\subsection{Fields}

A field is a set $\mathbb{F}$ defined under multiplication and division with the
following properties:

\begin{itemize}
  \item \textbf{Associativity} under multiplication and division
  \item \textbf{Commutativity} under multiplication and division
  \item $\mathbb{F}$ contains an \textbf{identity} under multiplication and division
  \item All elements in $\mathbb{F}$ contain an \textbf{inverse} under addition and
        multiplication (except $0$ under multiplication)
  \item The defined multiplication is \textbf{distributive} across
        the defined addition.
\end{itemize}

\subsection{Vector Spaces}

A group $(V, +_{V})$ ($+_{V}$ denotes addition defined with respect to the
set $V$ as it can be ambigious in some cases) is a vector space over the field
$\mathbb{F}$ if the following holds (let $v, w \in V$, $\lambda, \mu \in
  \mathbb{F}$):

\begin{itemize}
  \item $(V, +_{V})$ is \textbf{abelian}
  \item $V$ is \textbf{closed under multiplication} with elements in
        $\mathbb{F}$
  \item $\lambda(v +_{V} w) = \lambda v + \lambda w$
  \item $(\lambda + \mu)v = \lambda v +_{V} \mu v$
  \item $(\lambda \mu)v = \lambda (\mu v)$
  \item $fv = v$ where $f$ is the \textbf{multiplicative identity} of
        $\mathbb{F}$.
\end{itemize}

\subsection{Subspaces}

Let $V$ be a vector space over $\mathbb{F}$, $U \subseteq V$ is a subspace if
the following properties hold:

\begin{itemize}
  \item $U$ is \textbf{non-empty}
  \item $U$ is \textbf{closed} under the \textbf{addition} defined by $V$
  \item $U$ is \textbf{closed} under the \textbf{multiplication} defined by $V$.
\end{itemize}

\textit{Some notes on subspaces:}

\begin{itemize}
  \item Subspaces are vector spaces
  \item The intersection of subspaces is a subspace
  \item The span of any non-empty subset of a given vector space is a subspace.
\end{itemize}

\subsection{Linear Maps}

For $V, W$ vector spaces over $\mathbb{F}$, the map $T: V \to W$ is called linear
if the following properties hold (let $u, w \in V$, $\lambda \in \mathbb{F}$):

\begin{itemize}
  \item $T(u + v) = T(u) + T(v)$
  \item $T(\lambda u) = \lambda T(u)$.
\end{itemize}

\textit{A note, for a linear map ($T: V \to W$), if $V = W$, $T$ is sometimes
  referred to as a linear \textbf{operator}. Also, composed linear maps are also linear maps.}

\subsection{The Kernel and Image}

For a linear map ($T: V \to W$), the kernel is defined as follows:
\begin{align*}
  \Ker{T} = \{v \in V : T(v) = 0\}.
\end{align*}
The image is defined as follows:
\begin{align*}
  \Ima{T} = \{w \in W : \exists v \in V \text{ with } T(v) = w\}.
\end{align*}

\textit{Some notes on linear maps (let $T: V \to W$ be a linear map):}

\begin{itemize}
  \item The kernel and image of $T$ are subspaces of $V$ and $W$ respectively
  \item For $U \subseteq V$, $T(U)$ is also a subspace (but of $W$ instead of $V$).
\end{itemize}

\subsection{Bases and Dimension}

\subsubsection{Definition of linear independence}

For $V$ a vector space, with $S \subseteq V$, let $s_1, s_2, ... \in S$,

\begin{itemize}
  \item $S$ is linearly independent if $\sum_{n = 1}^{|S|} \lambda_n s_n = 0
          \Longleftrightarrow \lambda_1 = \lambda_2 = \cdots = \lambda_{|S|} = 0$
  \item S is linearly dependent if it's not linearly independent.
\end{itemize}

A result of linear dependence is that for a linear dependent set $S$, there
exists $s \in S$ such that $\Spa(S) = \Spa(S\backslash\{s\})$.

\vspace{\baselineskip}

\textit{A note, if $S$ is linearly dependent, there's a vector in $S$ such
  that it can be written as the sum of other vectors in $S$.}

\subsubsection{Definition of a basis}

For a vector space $V$, we say $S \subseteq V$ is a basis of $V$ if:
\begin{itemize}
  \item $S$ spans $V$
  \item $S$ is linearly independent.
\end{itemize}

\subsubsection{Properties of bases}

Let $V$ be a vector space:

\begin{itemize}
  \item For $v \in V$, $B$ a basis for $V$, $v$ can be written uniquely
        as a linear combination of vectors in $B$
  \item $V$ is finitely dimensional if $|B| < \infty$
  \item If $V$ is finitely dimensional, there must exist a basis of $V$.
\end{itemize}

For $V$ a vector space with $S \subseteq V$ a linearly independent set.
$S$ can be 'extended' to a basis of $V$. If $S$ spans $V$, it's
already a basis. If not, we add a vector from $V\backslash\Spa{S}$.
We can do this iteratively until we have a basis.

\subsubsection{Definition of dimension}

For a vector space $V$ with a basis $B$, the order of $B$ is the dimension
of $V$, all bases of $V$ share the same order. This is denoted by
$\dim{V} := |B|$.

\subsubsection{Properties of dimension}

Let $V$ be a finite dimensional vector space with $U, S \subseteq V$
where $U$ is a subspace:

\begin{itemize}
  \item $S$ is linearly independent $\Rightarrow |S| < \dim{V}$
  \item $\Spa{S} = V \Rightarrow |S| \geq dim{V}$
  \item $\Spa{S} = V$ and $|S| = \dim{V}
          \Rightarrow S$ is a basis of $V$.
  \item $\dim{U} \leq \dim{V}$
  \item $\dim{U} = \dim{V} \Rightarrow U = V$
\end{itemize}

\subsection{Direct Sums}

\subsubsection{Definition of a sum}

For $V$ a vector space over $\mathbb{F}$ with $U, W \subseteq V$
subspaces, we define their addition as follows:
\begin{align*}
  U + W = \{u + w : u \in U, w \in W\}.
\end{align*}

\subsubsection{Definition of a direct sum}

For $V$ a vector space over $\mathbb{F}$ with $U, W \subseteq V$
subspaces satisfying $U \cap W = \{0\}$, the addition of 
$U$ and $W$ ($U + W$) is called a direct sum denoted by:
\begin{align*}
  U \oplus W.
\end{align*}

\textit{So, when subspaces don't intersect, their addition is
called a direct sum as they are disjoint.}

\subsubsection{Decomposition of vector spaces}

For $V$ a vector space over $\mathbb{F}$ with $U, W \subseteq V$
subspaces satisfying $U \cap W = \{0\}$, we have that for all $v$
in $U \oplus W$:
\begin{align*}
  v = u + w \, (\text{for some } u \in U, \, w \in W).
\end{align*}

\subsubsection{Dimension of direct summed subspaces}

For $V$ a vector space over $\mathbb{F}$ with $U, W \subseteq V$
finite dimensional subspaces satisfying $U \cap W = \{0\}$:
\begin{align*}
  \dim(U \oplus W) = \dim(U) + \dim(W)
\end{align*}

\subsubsection{Complements of subspaces}

For $V$ a finite dimensional vector space over $\mathbb{F}$ 
with $U \subseteq V$ a subspace, we have that there exists
$W \subseteq V$ a subspaces such that:
\begin{itemize}
  \item $U \cap W = \{0\}$
  \item $U \oplus W = V$,
\end{itemize}
this is the complement of $U$ in $V$.

\subsection{The Rank-Nullity Theorem}

\subsubsection{Definition of rank and nullity}

For $V, W$ vector spaces over $\mathbb{F}$ and $T : V \to W$ a 
linear map, we define:
\begin{itemize}
  \item \textbf{Rank}: $\Rank(T) = \dim(\Ima(T))$
  \item \textbf{Nullity}: $\Null(T) = \dim(\Ker(T))$.
\end{itemize}

\subsubsection{The rank-nullity theorem}

For $V, W$ finite dimensional vector spaces over $\mathbb{F}$ 
and $T : V \to W$ a linear map, we can say:
\begin{align*}
  \Rank(T) + \Null(T) = \dim(V).
\end{align*}

\subsection{Injectivity and Surjectivity}

For $V, W$ vector spaces over $\mathbb{F}$ and $T : V \to W$ a 
linear map, we can say:
\begin{itemize}
  \item $T$ injective $\Leftrightarrow \Null(T) = 0$
  \item $T$ surjective $\Leftrightarrow \Rank(T) = \dim(W)$
  \item $T$ injective and $S \subseteq V$ linearly independent
  $\Rightarrow$ $T(S) \subseteq W$ is linealy independent
  \item $T$ surjective and $S \subseteq V$ spans $V$ 
  $\Rightarrow$ $T(S)$ spans $W$
  \item $\dim(W) > \dim(V) \Rightarrow T$ is not surjective 
  (you can't have surjective maps from 2D to 3D)
  \item $\dim(W) < \dim(V) \Rightarrow T$ is not injective
  \item $\dim(W) = \dim(V)$ means injectivity and
  surjectivity imply each other (you can't have one without
  the other).
\end{itemize}

\subsection{Projections}

\subsubsection{Definition of a projection}

For a vector space $V$, $P : V \to V$ a linear map, we say $P$
is a projection if $P^2 = P$.

\subsubsection{Relation to the rank-nullity theorem}

For a finite dimensional vector space $V$, $P : V \to V$ a projection,
we have:
\begin{align*}
  V = \Ker(P) \oplus \Ima(P)
\end{align*}

\subsubsection{The decomposition projection}

For $V$ a vector space over $\mathbb{F}$ with $U, W \subseteq V$
subspaces satisfying $U \cap W = \{0\}$, we can define a projection
as follows:
\begin{align*}
  P(v) = u \text{ where } v = u + w \text{ for some } u \in U, w \in W.
\end{align*}

\subsection{Isomorphisms}

\subsubsection{Definition of an isomorphism}

An isomorphism is a bijective linear map. It's domain and codomain
are isomorphic.

\subsubsection{Dimension of the domain and codomain}

For two finite dimensional vector spaces $V, W$:
\begin{gather*}
  \exists \, T : V \to W \text{ an isomorphism } \Leftrightarrow
  \dim(V) = \dim(W)
\end{gather*}

\subsection{Change of Bases}

\subsubsection{Method of changing basis}

For $V$ a vector space over $\mathbb{F}$, with $A, B \subseteq V$
bases, we can define a matrix to convert between these bases 
$C_{AB} = (c_{ij})$:
\begin{gather*}
  C_{AB} \text{ converts from $B$ to $A$ so we write $A$ in terms of $B$:} \\
  \text{Let } A = \{a_1, a_2, \ldots, a_n\} \\
  \text{Let } B = \{b_1, b_2, \ldots, b_n\} \\ \\
  \text{We have:} \\
  a_1 = c_{11}b_1 + c_{21}b_2 + \cdots + c_{n1}b_n \\
  a_2 = c_{12}b_1 + c_{22}b_2 + \cdots + c_{n2}b_n \\
  \cdots \\
  a_n = c_{1n}b_1 + c_{2n}b_2 + \cdots + c_{nn}b_n \\ \\
  \text{Leading to the matrix (note the transpose):} \\
  C_{AB} = \begin{pmatrix}
    c_{11} & c_{12} & \cdots & c_{1n} \\
    c_{21} & c_{22} & \cdots & c_{2n} \\
    \vdots & \vdots & \ddots & \vdots \\
    c_{n1} & c_{n2} & \cdots & c_{nn} \\
  \end{pmatrix}
\end{gather*}

\newpage

\subsubsection{Properties of the change of basis matrix}

For $A, B, X$ bases of a vector space $V$:
\begin{itemize}
  \item $C_{AA} = I$ (the identity)
  \item $C_{AB} = C_{BA}^{-1}$
  \item $C_{AX}C_{XB} = C_{AB}$
\end{itemize}

\subsubsection{Example of change of basis}

\begin{gather*}
  \text{Take } V = \mathbb{R}^2 \\
  \text{Let } A = \left\{ \Spvek{1;0}, \Spvek{0;1} \right\} \\
  \text{Let } B = \left\{ \Spvek{1;1}, \Spvek{-1;1} \right\}
\end{gather*}
\begin{multicols}{2}
  \begin{gather*}
    \text{For } C_{AB} \text{ we write $A$ in terms of $B$}: \\
    \Spvek{1;0} = (1/2)\Spvek{1;1} + (-1/2)\Spvek{-1;1} \\
    \Spvek{0;1} = (1/2)\Spvek{1;1} + (1/2)\Spvek{-1;1} \\ \\
    \text{So, after transposing, we get:} \\
    C_{AB} = \begin{pmatrix}
      1/2 & 1/2 \\
      -1/2 & 1/2 \\
    \end{pmatrix} \\
    \text{You can check for yourself that:} \\
    C_{AB}(b_1) = a_1 \\
    C_{AB}(b_2) = a_2 \\ \\
    \text{Or rather:} \\
    C_{AB}\Spvek{1;1} = \Spvek{1;0} \\
    C_{AB}\Spvek{-1;1} = \Spvek{0;1}   
  \end{gather*}
  
  \columnbreak
  
  \begin{gather*}
    \text{For } C_{BA} \text{ we write $B$ in terms of $A$}: \\
    \Spvek{1;1} = (1)\Spvek{1;0} + (1)\Spvek{0;1} \\
    \Spvek{-1;1} = (-1)\Spvek{1;0} + (1)\Spvek{0;1} \\ \\
    \text{So, after transposing, we get:} \\
    C_{BA} = \begin{pmatrix}
      1 & -1 \\
      1 & 1 \\
    \end{pmatrix} \\
    \text{You can check for yourself that:} \\
    C_{BA}(a_1) = b_1 \\
    C_{BA}(a_2) = b_2 \\ \\
    \text{Or rather:} \\
    C_{BA}\Spvek{1;0} = \Spvek{1;1} \\
    C_{BA}\Spvek{0;1} = \Spvek{-1;1}
  \end{gather*}
\end{multicols}

\newpage

\subsection{Linear Maps and Matrices}

\subsubsection{Definition of matrices of linear maps}

For $V, W$ vector spaces over $\mathbb{F}$ with $\dim(V) = n$ and
$\dim(W) = m$ and $T: V \to W$ a linear map. For each choice
of basis:
\begin{itemize}
  \item $B = \{b_1, b_2, \ldots, b_m\} \subseteq V$
  \item $A = \{a_1, a_2, \ldots, a_n\} \subseteq W$,
\end{itemize}
we can associate a matrix to $T$ (maps from $V$ to $W$ implying $B$ to $A$):
\begin{align*}
  M_{AB}(T) = (t_{ij}) \in M_{m, n}(\mathbb{F}),
\end{align*}
with each $t_{ij}$ defined as (write $T(B)$ in terms of $A$):
\begin{gather*}
  T(b_1) = t_{11}a_1 + t_{21}a_2 + \cdots + t_{m1}a_m \\
  T(b_2) = t_{12}a_1 + t_{22}a_2 + \cdots + t_{m2}a_m \\
  \cdots \\
  T(b_n) = t_{1n}a_1 + t_{2n}a_2 + \cdots + t_{mn}a_m.
\end{gather*}
Similarly to the change of basis matrices, note the transpose of
the values.

\subsubsection{Example of matrices of linear maps}

Define the following:
\begin{align*}
  B &= \left\{ \Spvek{1;0}, \Spvek{0;1} \right\} 
    \subseteq \mathbb{R}^2 \\
  A &= \left\{ 1 \right\} \subseteq \mathbb{R} \\
  T &: \mathbb{R}^2 \to \mathbb{R}; \; \Spvek{x; y} \mapsto 2x
\end{align*}
Since we are mapping from $\mathbb{R}^2$ to $\mathbb{R}$, our 
matrix will map from the basis $B$ to the basis $A$:
\begin{gather*}
M_{AB}(T) = (t_{ij}) \in M_{1, 2}(\mathbb{R}).
\end{gather*}
So, we write $T(B)$ in terms of $A$:
\begin{align*}
  \begin{rcases*}
    T \Spvek{1;0} = 2 = 2(1) \\
    T \Spvek{0;1} = 0 = 0(1)
  \end{rcases*} M_{AB}(T) = \begin{pmatrix}
    2 & 0 \\
  \end{pmatrix}
\end{align*}

\newpage

\subsubsection{Composition of matrices of linear maps}

For $U, V, W$ vector spaces over $\mathbb{F}$, $S : U \to V$, 
$T : V \to W$ linear maps, let $A \subseteq U$, $B \subseteq V$
and, $C \subseteq W$ be bases. We have:
\begin{align*}
  M_{CA}(T \circ S) = M_{CB}(T)M_{BA}(S).
\end{align*}

\subsubsection{Change of basis for matrices of linear maps}

For $V, W$ vector spaces over $\mathbb{F}$, $T : V \to W$ a linear 
map, let $A, A' \subseteq V$ and $B, B' \subseteq W$ be bases. 
We have:
\begin{align*}
  M_{B'A'}(T) = C_{B'B}M_{BA}(T)C_{AA'}.
\end{align*}

\subsubsection{Matrices of linear maps and the determinant}

For $V$ a vector space with $T : V \to V$ a linear map:
\begin{itemize}
  \item For any choice of basis $B$, $\det(M_{BB}(T))$ doesn't change
  so we define $\det(T) = \det(M_{BB}(T))$
  \item If $V$ is finite dimensional, $T$ is an isomorphism if
  $\det(T) \neq 0$.
\end{itemize}

\section{Eigenvalues and Eigenvectors}

\subsection{Definition of an Eigenvalue and Eigenvector}

For a vector space $V$ over $\mathbb{F}$ and $T : V \to V$ a linear
map, if we have $v$ in $V$ such that $v \neq 0$ and $T(v) = \lambda v$
we say $v$ is an eigenvector with eigenvalue $\lambda$.

\subsection{Eigenvector Bases and Matrices of Linear Maps}

For a vector space $V$ over $\mathbb{F}$ with dimension $n$ and 
$T : V \to V$ a linear map, if there exists 
$B = \{v_1, \ldots, v_n\}$ a basis for $V$ of eigenvectors of $T$
with eigenvalues $\{\lambda_1, \ldots, \lambda_n\}$ then:
\begin{align*}
  M_{BB}(T) = \Diag(\lambda_1, \ldots, \lambda_n).
\end{align*}

\subsection{Linear Independence of Eigenvectors}

If we have two eigenvectors with different eigenvalues, they are
linearly independent.

\subsection{Characteristic Polynomials}

\subsubsection{Definition of a characteristic polynomial}

For a vector space $V$ with $T:V \to V$ a linear map, we define
the characteristic polynomial $P$ as a polynomial such that:
\begin{align*}
  P(\lambda) = 0 \Rightarrow \lambda \text{ is an eigenvalue of } T.
\end{align*}
The set of eigenvalues of $T$ (and, equivalently, the set of roots 
of $P$) is called the spectrum of $T$ ($\Spec(T)$).

\subsubsection{Derivation of the characteristic polynomial}

For a finite dimensional vector space $V$ over $\mathbb{F}$ 
with $T:V \to V$ a linear map, let $\lambda$ be in $\mathbb{F}$:
\begin{gather*}
  \lambda \text{ is an eigenvalue of } T \\
  \Longleftrightarrow \\
  \det(T - \lambda I) = 0.
\end{gather*}
So, we can define the characteristic polynomial $P$ as follows:
\begin{gather*}
  P(\lambda) := \det(T - \lambda I).
\end{gather*}

\subsubsection{Eigenspaces}

For $V$ a vector space over $\mathbb{F}$, an eigenspace for an 
eigenvalue $\lambda$ in $\mathbb{F}$ of a linear map $T:V \to V$
is defined as:
\begin{align*}
  V_{\lambda} := \Ker(T - \lambda I).
\end{align*}
For $\Spec(T) = \{\lambda_1, \ldots, \lambda_n\}$, we have that:
\begin{gather*}
  V = V_{\lambda_1} \oplus \cdots \oplus V_{\lambda_n} \\
  \Longleftrightarrow \\
  T \text{ has a basis of eigenvectors}.
\end{gather*}

\newpage

\subsubsection{Calculating characteristic polynomials}

In general, with a $n$-dimensional vector space $V$ over 
$\mathbb{F}$, $T:V \to V$ a linear map, the characteristic
polynomial $P$ can be written as:
\begin{align*}
  P(\lambda) = (-1)^n\lambda^n + (-1)^{n-1}\Tr(T)\lambda^{n-1}
  + \cdots + \det(T).
\end{align*}
This is very simple in the $2 \times 2$ case:
\begin{align*}
  P(\lambda) = \lambda^2 - \Tr(T)\lambda + \det(T).
\end{align*}
It gets more complicated in the $3 \times 3$ case, consider $M$:
\begin{gather*}
  M = \begin{pmatrix}
    a & b & c \\
    d & e & f \\
    g & h & i \\
  \end{pmatrix}.
\end{gather*}
We calculate a value $\mu$:
\begin{gather*}
  \mu = \begin{pmatrix}
    a & b \\
    d & e \\
  \end{pmatrix} +
  \begin{pmatrix}
    e & f \\
    h & i \\
  \end{pmatrix} +
  \begin{pmatrix}
    a & c \\
    g & i \\
  \end{pmatrix}.
\end{gather*}
And we get the result:
\begin{align*}
  P(\lambda) = -\lambda^3 + \Tr(T)\lambda^2 - \mu\lambda + \det(T).
\end{align*}
Similar calculation for $4 \times 4$ matrices and upwards become
increasingly more complex.

\vspace{\baselineskip}

\textit{The calculation of $\mu$ may seem daunting at first but
it can be easily remembered as the $2 \times 2$ determinants of
the top left, bottom right, and corners.}

\subsubsection{Roots of characteristic polynomials}

We have that for a map $T$ with spectrum $\Spec(T) = \{\lambda_1,
\ldots, \lambda_k\}$, we can write the characteristic polynomial
as follows:
\begin{align*}
  P(\lambda) &= a_n\lambda^n + a_{n - 1}\lambda^{n-1} + \cdots + a_0 \\
  &= (\lambda - \lambda_1)^{m_1}(\lambda - \lambda_2)^{m_2}
    \cdots (\lambda - \lambda_k)^{m_k}
\end{align*}
The $m_i$ values are called the multiplicity of the roots. If we
are taking complex roots the sum of the $m_i$ values is
equal to $n$. This is because there's always $n$ complex roots (up
to multiplicity) but not always a similar amount of real roots.
This also means that there can never be more eigenvalues than the
dimension of the vector space.

\subsubsection{The characteristic polynomial and matrix properties}

In general, with a $n$-dimensional vector space $V$ over 
$\mathbb{F}$, $T:V \to V$ a linear map, the characteristic
polynomial $P$ can be written as:
\begin{align*}
  P(\lambda) = (-1)^n\lambda^n + (-1)^{n-1}\Tr(T)\lambda^{n-1}
  + \cdots + \det(T).
\end{align*}
So, by the properties of polynomials, we know that:
\begin{align*}
  \det(T) &= \text{ product of the roots of } P \\
  \Tr(T) &= \text{ sum of the roots of } P.
\end{align*}
As it's the characteristic polynomial, the roots are just the
eigenvalues. Let $\Spec(T) = \{\lambda_1, \ldots, \lambda_n\}$
(not necessarily distinct):
\begin{align*}
  \det(T) &= \prod_{i = 1}^k \lambda_k \\
  \Tr(T) &= \sum_{i = 1}^k \lambda_k
\end{align*}

\section{Inner Products}

\subsection{Definition of an Inner Product}

For $V$ a vector space over $\mathbb{C}$, an inner product on
$V$ is a map $\langle,\rangle : V \times V \to \mathbb{C}$ with
the following properties:
\begin{itemize}
  \item $\langle v, v \rangle \geq 0$
  \item $\langle v, v \rangle = 0 \Leftrightarrow v = 0$
  \item $\langle v, w \rangle = \overline{\langle w, v \rangle}$
  \item $\langle v, u + w \rangle 
    = \langle v, u \rangle + \langle v, w \rangle$
  \item $\langle v, \lambda w \rangle = \lambda \langle v, w \rangle
    \Rightarrow \langle \lambda v, w \rangle = \overline{\lambda}\langle v, w \rangle$.
\end{itemize}

\textit{When our values are real, we can just remove the conjugate
bar.}

\subsection{Inner Product Spaces}

A vector space paired with an inner product is called an inner
product space. If it's over the real/complex numbers we call
it a real/complex inner product space (respectively).

\subsection{The Norm}

For an inner product space $(V, \langle , \rangle)$, we define the
norm by:
\begin{align*}
  \|v\| := \sqrt{\langle v,v \rangle}
\end{align*}

\subsection{Properties of the Norm}

For an inner product space $(V, \langle , \rangle)$:

\begin{itemize}
  \item $|\langle v, w \rangle| \leq \|v\|\|w\|$
  \item $\|v\| = 0 \Leftrightarrow v = 0$
  \item $\|\lambda v\| = |\lambda|\|v\|$
  \item $\|v + w\| \leq \|v\| + \|w\|$.
\end{itemize}

\subsection{Matrix Derivation from the Inner Product}

For an inner product space $(V, \langle , \rangle)$, where
$\dim(V) = n$ with $T : V \to V$ a linear map. If we have
an orthonormal basis $B = \{v_1, \ldots, v_n\}$ we have:
\begin{align*}
  M_{BB}(T) = (a_{ij}) = (\langle v_i, T(v_j) \rangle)
\end{align*}

\section{Orthogonality}

\subsection{Definition of Orthogonal Vectors and Subspaces}

For an inner product space $(V, \langle , \rangle)$:
\begin{itemize}
  \item $v, w$ in $V$ are orthogonal $\Leftrightarrow \langle v,w \rangle = 0$ 
  \item $U, W$ subspaces of $V$ are orthogonal $\Leftrightarrow$ 
    $u$ and $w$ are orthogonal for all $u$ in $U$, $w$ in $W$.
\end{itemize}

\subsection{Orthogonal Complements}

For an inner product space $(V, \langle , \rangle)$ with
$U \subseteq V$ a subspace, we define the orthogonal complement
$U^{\perp}$ as follows:
\begin{gather*}
  U^{\perp} := \{v \in V : \langle v,u \rangle = 0, \forall u \in U\}.
\end{gather*}
A consequence of the definition is that $V = U \oplus U^{\perp}$.

\subsection{Pythagoras' Theorem}

For an inner product space $(V, \langle , \rangle)$ with $v, u$ in
$V$ such that $\langle v,u \rangle = 0$:
\begin{gather*}
  \|v + u\|^2 = \|v\|^2 + \|u\|^2 \\
  \text{or} \\
  \langle v+u,v+u \rangle = \langle v,v \rangle + \langle u,u \rangle
\end{gather*}

\subsection{Orthonormal Bases}

\subsubsection{Definition of an orthonormal basis}

For an inner product space $(V, \langle , \rangle)$ with a basis
$B = \{v_1, v_2, \ldots, v_n\}$. $B$ is an orthonormal basis
if:
\begin{align*}
  \langle v_i,v_j \rangle = \delta_{ij}.
\end{align*}

\subsubsection{Existence of an orthonormal basis}

All finite dimensional inner product spaces have an orthonormal
basis.

\subsubsection{Properties of orthonormal bases}

For an inner product space $(V, \langle , \rangle)$ with an
orthonormal basis $B = \{v_1, v_2, \ldots, v_n\}$ and
$v, w$ in $V$:
\begin{itemize}
  \item $v = \sum_{i = 1}^n \langle v_i,v \rangle v_i$
  \item $\langle v, w \rangle = \sum_{i = 1}^n 
    \overline{\langle v_i,v \rangle}\langle v_i,w \rangle$
  \item $\langle v, v \rangle = \sum_{i = 1}^n |\langle v_i, v \rangle|^2$.
\end{itemize}

\subsubsection{Orthogonal projections}

For an inner product space $(V, \langle , \rangle)$ a linear map
$P: V \to V$ is called an orthogonal projection if:
\begin{itemize}
  \item $P^2 = P$
  \item $\langle Pv,w \rangle = \langle v,Pw \rangle$.
\end{itemize}

\subsubsection{Constructing an orthogonal projection}

For an inner product space $(V, \langle , \rangle)$ with a
subspace $U \subseteq V$ with an orthonormal basis
$B = \{u_1, u_2, \ldots, u_k\}$, we have an orthogonal projection:
\begin{gather*}
  P : V \to V \\
  P(v) := \sum_{i = 1}^k \langle u_i, v \rangle u_i
\end{gather*}

\textit{So, we can write vectors as a linear combination of basis
vectors, if we do that and then remove some of those terms
we get this projection. We are writing vectors in terms of a
subset of basis vectors.}

\subsubsection{Properties of orthogonal projections}

For an inner product space $(V, \langle , \rangle)$ with an
orthogonal projection $P$, we have:
\begin{itemize}
  \item $(I - P)$ is also an orthogonal projection
  \item $V = \Ker(P) \oplus \Ima(P)$
  \item $\Ker(P) = \Ima(P)^\perp$
  \item $\|v - w\| \geq \|v - Pv\| \ (w \in \Ima(P))$
\end{itemize}

\section{Adjoint Operators}

\subsection{Definition of an Adjoint Operator}

For an inner product space $(V, \langle , \rangle)$ with 
$T : V \to V$ a linear map. We define the adjoint operator $T^*$ 
by the relation:
\begin{align*}
  \langle T^*v, w \rangle = \langle v, Tw \rangle.
\end{align*}

\subsection{Adjoint Matrices}

For an inner product space $(V, \langle , \rangle)$ with 
$T : V \to V$ a linear map with associated matrix 
$M_{BB}(T) = (a_{ij})$, the adjoint has associated adjoint matrix:
\begin{align*}
  M_{BB}(T^*) = (\overline{a_{ji}}).
\end{align*}

\textit{Take care to notice the transposition.}

\subsection{Properties of Adjoint Operators}

For an inner product space $(V, \langle , \rangle)$ with 
$S, T : V \to V$ linear maps:
\begin{itemize}
  \item $(S + T)^* = S^* + T^*$
  \item $(ST)^* = T^*S^*$
  \item $(T^*)^* = T$
  \item For $T$ invertible, $(T^*)^{-1} = (T^{-1})^*$.
\end{itemize}

\subsection{Types of Operators}

\subsubsection{Normal, unitary, and self-adjoint}

For an inner product space $(V, \langle , \rangle)$ with 
$T : V \to V$ a linear map. We say $T$ has:
\begin{itemize}
  \item The \textbf{normal} property if $T^*T = TT^*$
  \item The \textbf{unitary} property if $T^*T = I$
  \item The \textbf{self-adjoint} (hermitian) property if $T^* = T$
\end{itemize}

\textit{All unitary operators are normal. All self-adjoint operators
are normal.}

\subsubsection{Real associated matrices}

For an inner product space $(V, \langle , \rangle)$ with 
$T : V \to V$ a linear map with associated matrix $M$.
If $M$ is real $M^* = M^t$ (as it's the transpose and conjugate)
this leads to the following results:
\begin{itemize}
  \item Self-adjoint $\Leftrightarrow$ $M = M^t$ (symmetric)
  \item Unitary $\Leftrightarrow$ $M^{-1} = M^t$ (orthogonal)
\end{itemize}

\subsubsection{Eigenvalues of self-adjoint operators}

For an inner product space with a linear, self-adjoint map $T$, we
have that the eigenvalues of $T$ are all real.  

\subsubsection{Column vectors of unitary matrices}

For $M$ in $M_n(\mathbb{C})$, $M$ is unitary if and only if
the columns vectors of $M$ form an orthonormal basis.

\subsubsection{Properties of unitary operators}

For an inner product space $(V, \langle , \rangle)$ with 
$S, T : V \to V$ linear, unitary maps. We have:
\begin{itemize}
  \item $T^{-1}$, $T^*$, $ST$ unitary
  \item $\|Tv\| = \|v\|$
  \item All eigenvalues of $T$ have modulus $1$
\end{itemize}

\subsubsection{Eigenvalues of normal operators}

For an inner product space $(V, \langle , \rangle)$ with 
$T : V \to V$ a linear, normal map. If we have $v$ an eigenvector
of $T$ with eigenvalue $\lambda$ then $v$ is an eigenvector of $T^*$
with eigenvalue $\overline{\lambda}$.

\subsubsection{Eigenvectors of normal operators}

We had a result previously that stated that eigenvectors with
different eigenvalues were linearly independent. We now say for 
an inner product space with a linear, normal map $T$, we
have that for two eigenvectors of $T$, they are orthogonal if
their eigenvalues differ.

\subsubsection{Decomposition via normal operators}

For a finite dimensional, complex inner product space 
$(V, \langle , \rangle)$ with $T : V \to V$ a linear, normal map with
$\Spec(T) = \{\lambda_1, \ldots, \lambda_k\}$:
\begin{align*}
  V = V_{\lambda_1} \oplus \cdots \oplus V_{\lambda_k}
\end{align*}
So, if we have a finite dimensional, complex inner product 
space with a normal operator, we can write the space as a direct
sum of eigenspaces. Thus, the space has an orthonomal basis of eigenvectors.
Thus, the associated matrix to the operator is diagonalisable.

\subsubsection{Diagonalisation via normal operators}

For $M$ in $M_n(\mathbb{C})$ a normal matrix, we have that there
exists a unitary matrix $D$ in $M_n(\mathbb{C})$ such that the
columns of $D$ are the eigenvectors of $M$ and:
\begin{align*}
  D^*MD = \Diag(\lambda_1, \ldots, \lambda_n),
\end{align*}
where the diagonal matrix is formed by 
$\Spec(M) = \{\lambda_1, \ldots, \lambda_n\}$.

\vspace{\baselineskip}

\textit{So, all normal matrices are diagonalisable: 
self-adjoint (hermitian), unitary, real symmetric.}

\section{Real Matrices}

\subsection{Motivation}

We have worked with complex matrices of size $n$ as they always have
$n$ eigenvalues. If matrix is self-adjoint however, then all the
eigenvalues are real. So, it would make sense to consider real,
self-adjoint matrices (also known as real symmetric matrices).

\subsection{Orthogonal Matrices}

\subsubsection{Definition of an orthogonal matrix}

Unitary and real matrices are called orthogonal. Additionally,
we have $O$ in $M_n(\mathbb{R})$ orthogonal if the columns form
an orthonormal basis.

\subsubsection{Closure of orthogonal matrices}

For $O_1, O_2$ in $M_n(\mathbb{R})$ orthogonal matrices, $O_1O_2$ and
$O_1^{-1} = O_1^t$ are orthogonal.

\subsection{Diagonalisation of Real Symmetric Matrices}

For $M$ in $M_n(\mathbb{R})$ a symmetric, real matrix, we have that 
there exists a matrix $O$ in $M_n(\mathbb{R})$ orthogonal and:
\begin{align*}
  O^tMO = \Diag(\lambda_1, \ldots, \lambda_n),
\end{align*}
where the diagonal matrix is formed by 
$\Spec(M) = \{\lambda_1, \ldots, \lambda_n\}$.

\end{document}