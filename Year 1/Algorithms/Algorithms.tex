\documentclass[a4paper, 12pt, twoside]{article}
\usepackage[left = 3cm, right = 3cm]{geometry}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multicol}

\begin{document}

\title{Algorithms Notes}
\date{}
\author{\textit{paraphrased by} Tyler Wright}
\maketitle

\vfill

\textit{An important note, these notes are absolutely \textbf{NOT}
      guaranteed to be correct, representative of the course, or rigorous.
      Any result of this is not the author's fault.}

\newpage

\section{Bounding}

\subsection{Racetrack Principle}

For $f, g : \mathbb{N} \to \mathbb{N}$ functions, $n, k$ in $\mathbb{N}$
we have that:
\begin{align*}
      \begin{rcases*}
            f(k) \geq g(k) \\
            f'(n) \geq g'(n) \quad (\forall n \geq k)
      \end{rcases*} \Rightarrow
      f(n) \geq g(n) \quad (\forall n \geq k)
\end{align*}
\textit{If a function $f$ is greater than another function $g$ 
at a value $k$ and has a greater gradient for all values after
and including $k$, $f$ is greater than $g$ for all values 
after and including $k$.}

\subsection{Big $O$ Notation}

\subsubsection{Definition of the big $O$ notation}

For $g : \mathbb{N} \to \mathbb{N}$ a function, $0(g)$ is a set of
functions $f : \mathbb{N} \to \mathbb{N}$ such that each for
$f$ in $O(g)$:
\begin{gather*}
      \exists \, c \in \mathbb{R}, n_0 \in \mathbb{N}
      \text{ such that } \forall n \in N, \\
      (n \geq n_0) \Rightarrow (0 \leq f(n) \leq cg(n)).
\end{gather*}

\subsubsection{The big $O$ notation under multiplication}

For $f_1, f_2, g_1, g_2 : \mathbb{N} \to \mathbb{N}$ functions where:
\begin{itemize}
      \item $f_1 \in O(g_1)$
      \item $f_2 \in O(g_2)$,
\end{itemize}
we have that:
\begin{itemize}
      \item $f_1 + f_2$ is in $O(g_1 + g_2)$
      \item $f_1 \cdot f_2$ is in $O(g_1 \cdot g_2)$.
\end{itemize}

\subsubsection{Closure of the big $O$ notation}

For $g : \mathbb{N} \to \mathbb{N}$ a function, $O(g)$ is closed
under addition (this follows from the above).

\subsubsection{Polynomials and the big $O$ notation}

For $p : \mathbb{N} \to \mathbb{N}$ a polynomial of degree $k$,
$p$ is in $O(n^k)$.

\subsection{$\Theta$ Notation}

\subsubsection{Definition of the $\Theta$ notation}

For $g : \mathbb{N} \to \mathbb{N}$ a function, $\Theta(g)$ is a set of
functions $f : \mathbb{N} \to \mathbb{N}$ such that each for
$f$ in $\Theta(g)$:
\begin{gather*}
      \exists \, c_0, c_1 \in \mathbb{R}, n_0 \in \mathbb{N}
      \text{ such that } \forall n \in N, \\
      (n \geq n_0) \Rightarrow (0 \leq c_1g(n) \leq f(n) \leq c_2g(n)).
\end{gather*}
\textit{$f$ is sandwiched by multiples of $g$.}

\subsubsection{Equivalency of the $\Theta$ notation}

For $f, g : \mathbb{N} \to \mathbb{N}$ functions:
\begin{gather*}
      f \in \Theta(g) \Longleftrightarrow g \in \Theta(f).
\end{gather*}

\subsubsection{$\Theta$ and $O$ notation}
For $f, g : \mathbb{N} \to \mathbb{N}$ functions:
\begin{gather*}
      f \in \Theta(g) \Longleftrightarrow f \in O(g).
\end{gather*}
\textit{Which also means $g \in O(f)$ by the above equivalency.}

\subsubsection{Definition of the $\Omega$ notation}

For $g : \mathbb{N} \to \mathbb{N}$ a function, $\Omega(g)$ is a set of
functions $f : \mathbb{N} \to \mathbb{N}$ such that each for
$f$ in $\Omega(g)$:
\begin{gather*}
      \exists \, c \in \mathbb{R}, n_0 \in \mathbb{N}
      \text{ such that } \forall n \in N, \\
      (n \geq n_0) \Rightarrow (0 \leq cg(n) \leq f(n)).
\end{gather*}

\subsubsection{Equivalency of the $\Omega$ notation}

For $f, g : \mathbb{N} \to \mathbb{N}$ functions:
\begin{gather*}
      f \in \Omega(g) \Longleftrightarrow g \in O(f).
\end{gather*}

\newpage

\section{Runtime}

\subsection{Best-case Runtime}

Considering all the inputs for a given algorithm, the best-case
runtime is the runtime of the input that the algorithm takes 
the least amount of time to process.

\subsection{Worst-case Runtime}

Considering all the inputs for a given algorithm, the worst-case
runtime is the runtime of the input that the algorithm takes 
the most amount of time to process.

\subsection{Average Runtime}

Considering all the inputs for a given algorithm, the average
runtime is the average of the runtimes of all the inputs.

\section{Data Structures}

\subsection{Trees}

\subsubsection{Definition of a tree}

A tree $T$ of size $n$ is defined as $T = (V, E)$ where:
\begin{gather*}
      V = \{v_1, \ldots, v_n\} \text{ is a set of nodes} \\
      E = \{e_1, \ldots, e_{n - 1}\} \text{ is a set of edges},
\end{gather*} 
with the properties that for $i$ in $\{1, \ldots, n - 1\}$, 
$j, k$ in $\{1, \ldots, n\}$ with $j \neq k$ we have 
$e_i = \{v_j, v_k\}$, and for all $i$ in $\{1, \ldots, n\}$, 
there exists $j$ in $\{1, \ldots, n - 1\}$ such that $v_i$ 
is in $e_j$.

\vspace{\baselineskip}

\textit{Basically, we have $n$ nodes and $n - 1$ edges where each
node has least one edge and the edges can't branch between 
identical nodes.}

\subsubsection{Rooted trees}

A rooted tree is defined as $T = (v, V, E)$ where $T = (V, E)$ is a
tree and $v$ in $V$ is the root of $T$.

\subsubsection{Leaves and internal nodes}

A leaf in a tree is a node with exactly one incident edge. If a node
isn't a leaf, it's an internal node.

\subsubsection{Other definitions}

\begin{itemize}
      \item The \textbf{parent} of a node is the closest node on
      the path from the node to the root (the root has no parent)
      \item The \textbf{children} of a node are all its neighbours
      barring its parent
      \item The \textbf{height} of a tree is the length of the
      longest path connecting the root with a leaf
      \item The \textbf{degree} of a node is the number edges
      incident on the node
      \item The level of a node is the length of the unique path
      from the root to it plus one.
\end{itemize}

\subsubsection{$k$-ary trees}

A $k$-ary tree is a rooted tree where each node has at most $k$
children. A $k$-ary tree is:
\begin{itemize}
      \item \textbf{Full} if all internal nodes have exactly 
      $k$ children
      \item \textbf{Complete} if all levels except the last are full.
      \item \textbf{Perfect} if all levels are full.
\end{itemize}
Complete and perfect $k$-ary trees have heights of $O(\log_k(n))$.

\subsection{Priority Queues}

\subsubsection{Definition of a priority queue}

Priority queues are data structures that allow for the creation
of a data structure from an array of values and the extraction 
of said data structure's maximum.

\subsubsection{A tree-oriented priority queue}

An array could be interpreted by a priority queue as a complete 
binary tree with the indices from top to bottom, left to right. 
So, where applicable, for a node index $i$, the parent of the 
node has index $\lfloor i / 2 \rfloor$ and the children have 
indices $2i$ and $2i + 1$.

\subsubsection{Heaps}

If we have a tree-oriented priority queue and we add the condition
(heap property) that the values of nodes must be greater than
their children we get a tree with the maximum at the root. We
call this tree a heap.

\subsubsection{Producing a heap}

Given a binary tree, we can transform it into a heap using a
\texttt{heapify} function. This function takes a node and its children
and ensures the maximum value of these nodes lies in the parent node.

\begin{center}
      \begin{tabular}{ || c | p{8.5cm} || }
            \hline
                  \textbf{Input} & A binary tree and an index \\
                  \textbf{Output} & A binary tree such that the value 
                        at the given index is greater than or equal 
                        to the values of its children \\
            \hline\hline
                  \textbf{Runtime} & $O(\log_2(n))$ \\
            \hline
      \end{tabular}
\end{center}

Using this, we traverse through the internal nodes, performing
\texttt{heapify}. If the function makes a change, we perform
\texttt{heapify} on the child node that was swapped with.
This produces a heap.

\vspace{\baselineskip}

So, we can see that the heap building function has the following
properties:

\begin{center}
      \begin{tabular}{ || c | p{8.5cm} || }
            \hline
                  \textbf{Input} & A binary tree \\
                  \textbf{Output} & A heap \\
            \hline\hline
                  \textbf{Runtime} & $O(n\log_2(n))$ \\
            \hline
      \end{tabular}
\end{center}

\newpage

\section{Searching}

\subsection{Linear Search}

\subsubsection{Information on Linear Search}

\begin{center}
      \begin{tabular}{ || c | p{8.5cm} || }
            \hline
                  \textbf{Input} & An array of integers and an integer
                        $x$ both in $[0, n)$ for some $n$ in $\mathbb{N}$ \\
                  \textbf{Output} & $1$ if $x$ is in the array, $0$
                        otherwise \\
            \hline\hline
                  \textbf{Best-case Runtime} & $O(1)$ \\
                  \textbf{Average Runtime} & $O(n)$ \\
                  \textbf{Worst-case Runtime} & $O(n)$ \\
            \hline
      \end{tabular}
\end{center}

\subsubsection{Process of Linear Search}

Iterate through the array comparing the input value with the current
array value. If it's equal, return $1$. If we reach the end of the
array, return $0$.

\subsection{Binary Search}

\subsubsection{Information on Binary Search}

\begin{center}
      \begin{tabular}{ || c | p{8.5cm} || }
            \hline
                  \textbf{Input} & A sorted array of integers and 
                        an integer $x$ both in $[0, n)$ for some $n$
                        in $\mathbb{N}$ \\
                  \textbf{Output} & $1$ if $x$ is in the array, $0$
                        otherwise \\
            \hline\hline
                  \textbf{Best-case Runtime} & $O(1)$ \\
                  \textbf{Average Runtime} & $O(\log_2(n))$ \\
                  \textbf{Worst-case Runtime} & $O(\log_2(n))$ \\
            \hline
      \end{tabular}
\end{center}

\subsubsection{Process of Binary Search}

Look at the middle value of the array, if equal to the input value
then return $1$. If the value is greater than our input value, repeat
the process with the lesser half of the array. Otherwise, repeat with
the greater half of the array.

\vspace{\baselineskip}

\textit{This works because the array is sorted.}

\section{Sorting}

\subsection{Properties of Sorting Algorithms}

\subsubsection{In place}

A sorting algorithm is in place if at any moment at most $O(1)$
array elements are stored outside the array.

\subsubsection{Stable}

A sorting algorithm is stable if any pair of equal values appear
in the same order in the sorted array (this may be important
if this value is tied to some overarching data structure).

\subsection{Lower Bound for Comparison-Based Sorting}

When sorting comparatively, we can only sort an array length
$n$ at best in $O(n\log(n))$ time.

\subsection{Insertion Sort}

\subsubsection{Information on Insertion Sort}

\begin{center}
      \begin{tabular}{ || c | p{8.5cm} || }
            \hline
                  \textbf{Input} & An array of integers length $n$
                        in $\mathbb{N}$ \\
                  \textbf{Output} & An ascending, sorted array \\
            \hline\hline
                  \textbf{Best-case Runtime} & $O(n)$ \\
                  \textbf{Average Runtime} & $\Theta(n^2)$ \\
                  \textbf{Worst-case Runtime} & $O(n^2)$ \\
            \hline\hline
                  \textbf{In place} & \checkmark \\
                  \textbf{Stable} & \checkmark \\
            \hline
      \end{tabular}
\end{center}

\subsubsection{Process of Insertion Sort}

Iterate through the array $A$, when at position $i$, place $A[i]$
into the array at some index in $\{0, \ldots, i\}$ such that
$A[0, i]$ is sorted.

\subsection{Merge Sort}

\subsubsection{Information on Merge Sort}

\begin{center}
      \begin{tabular}{ || c | p{8.5cm} || }
            \hline
                  \textbf{Input} & An array of integers length $n$
                        in $\mathbb{N}$ \\
                  \textbf{Output} & An ascending, sorted array \\
            \hline\hline
                  \textbf{Best-case Runtime} & $O(n\log_2(n))$ \\
                  \textbf{Average Runtime} & $O(n\log_2(n))$ \\
                  \textbf{Worst-case Runtime} & $O(n\log_2(n))$ \\
            \hline\hline
                  \textbf{In place} & $\times$ \\
                  \textbf{Stable} & \checkmark \\
            \hline
      \end{tabular}
\end{center}

\subsubsection{Process of Merge Sort}

If the array size is less than 3, reorder the elements and return.
Otherwise, split the array into two, perform merge sort on the two 
halves and combine them.

\subsection{Heap Sort}

\subsubsection{Information on Heap Sort}

\begin{center}
      \begin{tabular}{ || c | p{8.5cm} || }
            \hline
                  \textbf{Input} & An array of integers length $n$
                        in $\mathbb{N}$ \\
                  \textbf{Output} & An ascending, sorted array \\
            \hline\hline
                  \textbf{Runtime} & $O(n\log_2(n))$ \\
            \hline\hline
                  \textbf{In place} & \checkmark \\
                  \textbf{Stable} & $\times$ \\
            \hline
      \end{tabular}
\end{center}

\subsubsection{Process of Heap Sort}

Produce a heap from the array and extract the maximum from it
until it is empty (ensuring that it remains a heap between
extractions).

\newpage

\subsection{Quick Sort}

\subsubsection{Information on Quick Sort}

\begin{center}
      \begin{tabular}{ || c | p{8.5cm} || }
            \hline
                  \textbf{Input} & An array of integers length $n$
                        in $\mathbb{N}$ \\
                  \textbf{Output} & An ascending, sorted array \\
            \hline\hline
                  \textbf{Best-case Runtime} & $O(n\log_2(n))$ \\
                  \textbf{Average Runtime} & $O(n\log_2(n))$ \\
                  \textbf{Worst-case Runtime} & $O(n^2)$ \\
            \hline\hline
                  \textbf{In place} & \checkmark \\
                  \textbf{Stable} & $\times$ \\
            \hline
      \end{tabular}
\end{center}

\textbf{(Based on a random pivot selection procedure)}

\subsubsection{Process of Quick Sort}

Choose a pivot, move all values greater than the pivot into indices
greater than the pivot's and move all values less than the pivot
into indices less than the pivot. 

\vspace{\baselineskip}

This can be done by storing
the smallest index $i$ such that it's value has not been swapped
then iterating through the array (excluding the pivot) and 
checking if the value is less or equal to the pivot. If so, move it to 
position $i$ and increment $i$. Once at the end of the array,
the pivot can be moved into position $i$.

\vspace{\baselineskip}

Then, we perform quick sort on all values less than the pivot and
on all values greater than the pivot.

\subsection{Counting Sort}

\subsubsection{Information on Counting Sort}

\begin{center}
      \begin{tabular}{ || c | p{8.5cm} || }
            \hline
                  \textbf{Input} & An array of non-negative 
                        integers length $n$ in $\mathbb{N}$ \\
                  \textbf{Output} & An ascending, sorted array \\
            \hline\hline
                  \textbf{Runtime} & $O(n))$ \\
            \hline\hline
                  \textbf{In place} & $\times$ \\
                  \textbf{Stable} & \checkmark \\
            \hline
      \end{tabular}
\end{center}

\subsubsection{Process of Counting Sort}

Create an array $A$ with its length equal to the maximum of the input 
array. At each index, set the value equal to the amount of values 
less than the index in the input array. Then, we can fill our input array with the correct amount of
each integer in increasing order.

\subsection{Radix Sort}

\subsubsection{Information on Radix Sort}

\begin{center}
      \begin{tabular}{ || c | p{8.5cm} || }
            \hline
                  \textbf{Input} & An array of non-negative 
                        integers length $n$ in $\mathbb{N}$ \\
                  \textbf{Output} & An ascending, sorted array \\
            \hline\hline
                  \textbf{Runtime} & $O(n))$ \\
            \hline\hline
                  \textbf{In place} & \checkmark \\
                  \textbf{Stable} & \checkmark \\
            \hline
      \end{tabular}
\end{center}

\subsubsection{Process of Radix Sort}

Using a stable sorting algorithm, sort the least to the most
significant digits.

\section{Recurrences}

\subsection{Motivation}

For an increasing function $T: \mathbb{N} \to \mathbb{N}$ that
produces the worst-case runtime for an algorithm for an input
$n$. If we have that $T$ is defined in terms of itself:
\begin{align*}
      T(1) &= c_1 \\
      T(n) &= c_2T(f(n)) + c_3g(n),
\end{align*}
where $c_1, c_2, c_3$ are in $\mathbb{N}$ and 
$f, g: \mathbb{N} \to \mathbb{N}$. We would like to bound
this function.

\subsection{Substitution}

We can guess what bounds the function. Consider the following:
\begin{align*}
      T(1) &= c_1 \\
      T(n) &= 2T(\lfloor n / 2 \rfloor) + c_2n.
\end{align*}
Suppose we guess that $T(n) \leq C[n\log_2(n)]$, we will use induction
to prove our claim (as we are looking for an asymptotic comparison
we can take any $n$ in $\mathbb{N}$ as our base case):
\begin{center}
      Base case $(n = 2)$:
      \begin{align*}
            T(2) & = 2T(1) + 2c_2 = 2(c_1 + c_2) \\
            C[2\log_2(2)] & = 2C
      \end{align*} 
      So, for $C \geq (c_1 + c_2)$, $T(2) \leq C[2\log_2(2)]$.    
\end{center}
\begin{center}
      Inductive step, suppose the claim holds for all $n < k$:
      \begin{align*}
            T(k) & = 2T(\lfloor k / 2 \rfloor) + c_2(k) \\
            & \leq 2(C[k / 2]\log_2(k / 2)) + c_2(k) \\
            & = C[k\log_2(k / 2)] + c_2(k) \\
            & = Ck[\log_2(k) - 1] + c_2(k) \\
            & = Ck\log_2(k) + k(c_2 - C)
      \end{align*}
      So, for $C \geq c_2$, we have:
      \begin{align*}
            T(k) \leq C[k\log_2(k)].
      \end{align*}
      So, for $C \geq (c_1 + c_2)$, the claim holds for all $n \geq$ 2
      by induction.
\end{center}


\end{document}