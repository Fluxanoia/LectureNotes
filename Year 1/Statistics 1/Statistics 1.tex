\documentclass[a4paper, 12pt, twoside]{article}
\input{../../Packages.tex}

\def\lc{\left\lceil}   
\def\rc{\right\rceil}
\def\lf{\left\lfloor}   
\def\rf{\right\rfloor}

\DeclareMathOperator{\Mse}{mse}
\DeclareMathOperator{\Bias}{bias}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}

\begin{document}

\fluxtitle{Statistics 1}{}{}{true}{June 2019}{}
\newpage

\section{The Basics of Data Analysis}

\subsection{Samples}

A sample is a set of values observed from a simple random sample of
some size $n$ from a population where each sample member is chosen
\textbf{independently} of each other and each population member is
\textbf{equally likely} to be selected.

\vspace{\baselineskip}

Samples are usually written as $\{x_1, x_2, \ldots, x_n\}$ where
each $x_i$ represents an observed value. If the data is ordered,
the data is written as $\{x_{(1)}, x_{(2)}, \ldots, x_{(n)}\}$
(for numerical values, this is ascending order). So, in this
case, $x_{(1)}$ is always the minimum, $x_{(n)}$ is always
the maximum.

\subsection{Probability Density Functions}

For a sample $\{x_1, x_2, \ldots, x_n\}$, we can imagine each
datum as being distributed with some population distribution
$X$. As each datum is independent of all other observed values, we
can write the probability density of this sample as follows:
\begin{align*}
    f_X(x_1, x_2, \ldots, x_n) = \prod_{i = 1}^n f_X(x_i).
\end{align*}

\subsection{Measures of Central Tendency}

\subsubsection{Sample Median}

For a sample $X = \{x_1, x_2, \ldots, x_n\}$, we define the sample
median $M$ as follows:
\begin{align*}
    M(X) = \begin{cases}
        x_{(m + 1)}                     & \text{ for } n = 2m + 1 \\
        \frac{x_{(m)} + x_{(m + 1)}}{2} & \text{ for } n = 2m.
    \end{cases}
\end{align*}

\textit{Essentially, it equals the middle value or the average of the
    middle values. Also, it's important to note that the median is not
    sensitive to extreme values.}

\subsubsection{Sample Mean}

For a sample $X = \{x_1, x_2, \ldots, x_n\}$, we define the sample
mean $\overline{X}$ as follows:
\begin{align*}
    \overline{X} = \frac{1}{n}\left(\sum_{i = 1}^{n} x_i \right).
\end{align*}
This is easy to calculate even when combining samples. However, it is
sensitive to extreme values.

\subsubsection{Trimmed Sample Mean}

For a sample $X = \{x_1, x_2, \ldots, x_n\}$, we define the trimmed
sample mean $\overline{X_\Delta}$ for some percentage $\Delta\%$ as follows:
\begin{align*}
    \text{Let } k         & = \lf n \frac{\Delta}{100} \rf \\
    \text{Let } \tilde{X} & = \{x_{(k + 1)}, x_{(k + 2)},
    \ldots, x_{(n - k)}\}                                  \\
    \overline{X_\Delta}   & = \overline{\tilde{X}} \;
    (\text{the sample mean of } \tilde{X}).
\end{align*}

\textit{Basically, you remove the first and last $\Delta\%$
    of values and take the sample mean of the remaining values.}

\subsection{Measures of Spread}

\subsubsection{Sample Variance}

For a sample $X = \{x_1, x_2, \ldots, x_n\}$, we define the sample
variance $s^2$ as follows:
\begin{align*}
    s^2 & = \frac{\sum_{i = 1}^n (x_i - \bar{x}^2)}{n - 1}                  \\
        & = \frac{1}{n-1}\left(\sum_{i = 1}^{n}(x_i^2) - n\bar{x}^2\right).
\end{align*}
This measures how much the data varies.

\subsubsection{Hinges}

There are two hinge measures, lower ($H_1$) and upper ($H_3$):
\begin{align*}
    H_1 & = \text{ median of \{data values } \leq \text{ the median\}}  \\
    H_3 & = \text{ median of \{data values } \geq \text{ the median\}}.
\end{align*}

\subsubsection{Quartiles}

For a sample $X = \{x_1, x_2, \ldots, x_n\}$, there are two quartile
measures, lower ($Q_1$) and upper ($Q_3$). The formulae are long and
overly complicated so for $Q_1$:

\begin{itemize}
    \item Calculate $k = \frac{n + 1}{4}$
    \item If $k \in \mathbb{Z}$, $Q_1 = x_{(k)}$
    \item Otherwise, do linear interpolation between $x_{(\lf k\rf)}$
          and $x_{(\lf k+1\rf)}$
\end{itemize}

And similarly for $Q_3$:

\begin{itemize}
    \item Calculate $k = 3\left(\frac{n + 1}{4}\right)$
    \item If $k \in \mathbb{Z}$, $Q_3 = x_{(k)}$
    \item Otherwise, do linear interpolation between $x_{(\lf k\rf)}$
          and $x_{(\lf k+1\rf)}$
\end{itemize}

For large samples, the quartiles and hinges tend to be close to each
other.

\subsubsection{Interquartile Range (IQR)}

The IQR is the difference between $Q_3$ and $Q_1$ $(Q_3 - Q_1)$.

In this course, outliers are defined as more than
$\frac{3}{2}(\text{IQR})$ (or approx. $\frac{3}{2}(H_3 - H_1)$) from the median.

\subsubsection{Skewness}

We measure skewness by the distance of the hinges from the median.
If $H_3$ is further from the median than $H_1$, we have a longer
right tail. If the converse is true, we have a longer left tail.

\section{Assessing Fit}

\subsection{Quantiles of a Distribution}

For a distribution $X$ with cumulative distribution function
$F_X$, the quantiles of the distribution are defined as the set
of values:
\begin{align*}
    F_X^{-1} \left\{ \frac{1}{n+1}, \ldots, \frac{n}{n+1} \right\}.
\end{align*}

\textit{We use $n + 1$ on the denominator as $F_X^{-1}(1)$ can be
$\infty$.}

\vspace{\baselineskip}

The ordered sample is called the set of sample quantiles.

\newpage

\subsection{Quantile-Quantile (Q-Q) Plots}

These are the steps for constructing a Q-Q plot of a sample
$\{x_1, x_2, \ldots, x_n\}$ with cumulative distribution function
$F_X$:

\begin{itemize}
    \item Generate an estimate for the parameter(s)
          ($\hat\theta_1, \hat\theta_2, \ldots$)
    \item Compute the quantiles (the expected quantiles if the
          hypothesised model is correct)
    \item Plot each expected quantile against the sample quantile
          $(F_X^{-1}(\frac{k}{n + 1}; \hat\theta), x_{(k)})$.
\end{itemize}

What we would expect, if our hypothesis is correct, is that the
plotted points lie close to the line $y = x$. This is saying our
sample and expected quantiles are close together.

\subsection{Probability Plots}

These are similar to the Q-Q plots but plot the sample cumulative
probability against expected probability $(F_X(x_{(k)}), \frac{k}{n + 1})$.


\section{Estimation}

We have that a population is distributed with some distribution $X$
with a probability density function (PDF) $f_X$, cumulative
distribution function (CDF) $F_X$, and some parameters
$\{\theta_1, \ldots\}$. We can make guesses at the distribution of a sample and use tests to
verify that. But, to do these tests we need a valu for the parameters.
It's not practical to guess these, so we need to estimate them.

\subsection{Parameters}

We say $\hat{\theta}$ is an estimator for $\theta$ and define it as
a function of a sample $\{x_1, x_2, \ldots, x_n\}$:
\begin{align*}
    \hat{\theta}(x_1, x_2, \ldots, x_n).
\end{align*}

\subsection{Distribution Quantities}

From our estimated value of the distribution parameters, we can
calculate estimated values for distribution quantities like the
mean and variance. We consider $\tau$ a function of the parameter
that gives a distribution quantity:

\begin{itemize}
    \item \textbf{True quantity}: $\tau(\theta)$ where $\theta$
          is the true distribution parameter
    \item \textbf{Estimated quantity}:
          $\hat{\tau} = \tau(\hat{\theta})$ where $\hat{\theta}$ is our
          estimated parameter.
\end{itemize}

\section{Method of Moments Estimation}

\subsection{Definition of a Moment}

The $k$th moment of a probability distribution $X$ is defined as
follows:
\begin{align*}
    \mathbb{E}(X^k) := \int_{-\infty}^{\infty} x^k f_X(x) dx.
\end{align*}
Setting $k = 1$ gives us the familiar expectation of $X$:
\begin{align*}
    \mathbb{E}(X) = \int_{-\infty}^{\infty} x f(x) dx.
\end{align*}

\textit{In the discrete case, the integral is a sum.}

\vspace{\baselineskip}

We define the $k$th sample moment $m_k$ as follows:
\begin{align*}
    m_k = \frac{\sum_{i = 1}^{n} x_i^k}{n}.
\end{align*}

\textit{Or rather, the $k$th moment is the average value of $x^k$
    in the sample.}

\subsection{The Process}

By considering the a probability distribution $X$ with parameter $\theta$,
we can find functions for the moments of $X$ in terms of $\theta$. These
can be rearranged to give functions for $\theta$ in terms of the
moments. We can then use the sample moments to generate an estimate
for $\theta$ ($\hat\theta_{mom}$).

\subsection{Method of Moments on the Exponential}

Assume we have some population $X$ distributed according to the
Exponential with some parameter $\theta$. We say
$X \sim \text{Exp}(\theta)$.
\begin{align*}
                   & f_X(x) = \theta e^{-\theta x} \tag{x $>$ 0} \\
    \Rightarrow \; & \mathbb{E}(X) = \frac{1}{\theta}            \\
    \Rightarrow \; & \theta = \frac{1}{\mathbb{E}(X)}            \\
    \Rightarrow \; & \hat{\theta}_{mom} = \frac{1}{m_1}.
\end{align*}

\textit{If there were more parameters, we would have to consider
    greater moments of $X$.}

\section{Maximum Likelihood Estimation}

\subsection{The Process}

By considering the a probability distribution $X$ with parameter $\theta$,
we can find functions for the probability of events occuring in terms
of $\theta$. If we find where this function is maximised, it will give us
the value of $\theta$ that makes this sample most likely. This is the
maximum likelihood estimate ($\hat\theta_{mle}$).

\subsection{Optimisation of the Method}

Consider a sample $\{x_1, x_2, \ldots, x_n\}$ with distributions
$\{X_1, X_2, \ldots, X_n\}$, we call the likelihood function the
joint PDF of $\{X_1, X_2, \ldots, X_n\}$. We input our sample values
($L = f_{X_1, X_2, \ldots, X_n}(x_1, x_2, \ldots, x_n)$) which gives us
a function in terms of our unknown parameters $\theta_1, \theta_2, \ldots$.

\vspace{\baselineskip}

For $X_1, X_2, \ldots, X_n$ independent and identically distributed
sharing some distribution $X$, the joint PDF can be written as a
product of marginals:
\begin{align*}
    L & = f_{X_1, X_2, \ldots, X_n}(x_1, x_2, \ldots, x_n) \\
      & = f_X(x_1)f_X(x_2)\cdots f_X(x_n)                  \\
      & = \prod_{i = 1}^n f_X(x_i).
\end{align*}

We can take the natural logarithm of this likelihood function (the value
where it's maximised is preserved as the natural logarithm is increasing)
($\ell = ln(L)$). If, again, $X_1, X_2, \ldots, X_n$ are independent
and identically distributed sharing some distribution $X$:
\begin{align*}
    \ell & = \ln{\left(\prod_{i = 1}^n f_X(x_i)\right)} \\
         & = \sum_{i = 1}^n\left[\ln{(f_X(x_i))}\right]
\end{align*}
We know $\hat\theta_{mle}$ is the solution to:
\begin{align*}
    \frac{\partial}{\partial\theta}\ell(\theta) = 0.
\end{align*}
So, in the independent and identically distributed case:
\begin{equation*}
    \frac{\partial}{\partial\theta}\ell(\theta) = 0
    \Longleftrightarrow
    \sum_{i = 1}^n\left[\frac{\partial}{\partial\theta}\ln{(f_X(x_i))}\right]
    = 0.
\end{equation*}

\subsection{Multiple Parameters}

When finding the maximum likelihood estimate for multiple parameters, we
obtain multiple equations by partially differentiating $\ell$ by our
different parameters, giving an equation for each.

\subsection{Non-regular density}

If our function $L$ is piecewise, we may find that our maximum isn't where
the derivative is zero, but as the endpoints of the parts of the function.

\vspace{\baselineskip}

\textit{Consider the maximum of $f:\mathbb{R} \to \mathbb{R}$ where:}
\begin{align*}
    f(x) = \begin{cases}
        \theta^{-x} & \text{ for } x \geq 1 \\
        0           & \text{ otherwise.}
    \end{cases}
\end{align*}

\section{The Performance of Estimators}

\subsection{Variation of Estimators}

We can consider the distribution of an estimator to compare them. We
consider these main quantities:

\begin{itemize}
    \item $\Bias(\hat\theta) = \mathbb{E}(\hat\theta) - \theta$
    \item $\Mse(\hat\theta) = \mathbb{E}((\hat\theta - \theta)^2)$.
\end{itemize}

\textit{Bias is the quantity we expect our estimator to vary by from
    the true value. The MSE (mean squared error) is how much is varies.}

\vspace{\baselineskip}

We can rewrite the formula for the mean squared error as follows:
\begin{align*}
    \Mse(\hat\theta) = \Var(\hat\theta) + \Bias(\hat\theta)^2.
\end{align*}

\subsection{Method of Simulation}

If we have a distribution with known parameters $\theta_1, \theta_2,
    \ldots$, we can sample $N$ samples of size $n$ and use our estimators
to calculate estimates for these known parameters for each sample.

\vspace{\baselineskip}

From this, we can calculate the average error, sample variance, and
average squared error of each estimator. These quantities estimate
bias, variance, and mean squared error respectively.

\vspace{\baselineskip}

If we repeat this process for multiple estimators, we can compare
our estimators with these quantities.

\section{Central Limit Theorem}

\subsection{Definition of the Central Limit Theorem}

For $X_1, X_2, \ldots, X_n$ a random sample from a population with
mean $\mu = \mathbb{E}(X)$ and variance $\sigma^2 = \Var(X)$. Let
$\overline{X_n}$ be the sample mean. For $n$ large we have:
\begin{align*}
    \mathbb{P}\left( \sqrt{n}
    \left[\frac{\overline{X_n} - \mu}{\sigma}\right]
    \leq x \right) \simeq \mathbb{P}(\mathcal{N}(0, 1) \leq x) = \Phi(x).
\end{align*}
Or similarly:
\begin{align*}
    \overline{X_n} \simeq \mathcal{N}(\mu, \sigma^2/n).
\end{align*}

\subsection{Continuity Correction}

When using the Central Limit Theorem to approximate discrete random
variables, it is important to make a continuity correction. Let
$X_1, X_2, \ldots, X_n$ be samples from a discrete random variable
with sample mean $\overline{X_n}$, population mean $\mu$, and
population variance $\sigma^2$:
\begin{align*}
    \mathbb{P}\left( \sqrt{n}
    \left[\frac{\overline{X_n} - \mu}{\sigma}\right]
    \leq x \right) \simeq \mathbb{P}(\mathcal{N}(0, 1)
    < x + \frac{1}{2}) \\
    \mathbb{P}\left( \sqrt{n}
    \left[\frac{\overline{X_n} - \mu}{\sigma}\right]
    < x \right) \simeq \mathbb{P}(\mathcal{N}(0, 1)
    < x - \frac{1}{2})
\end{align*}

\section{A Reminder on Moment Generating Functions}

\subsection{Definition of a Moment Generating Function (MGF)}

For a random variable $X$, we define the moment generating function by:
\begin{align*}
    \mathcal{M}_X(t) := \mathbb{E}(e^{tX}) = \begin{cases}
        \int_{-\infty}^\infty e^{tX} f_X(x) \, dx & \text{for } X \text{ continuous} \\
        \sum_{x \in S} e^{tX} \mathbb{P}(X = x)   & \text{for } X \text{ discrete.}
    \end{cases}
\end{align*}

\subsection{Properties of a Moment Generating Function}

\subsubsection{Standard examples of moment generating functions}

For a random variable $X$:

\begin{itemize}
    \item $X \sim \mathcal{N}(\mu, \sigma^2) \Leftrightarrow
              \mathcal{M}_X(t) = \exp{(\mu t + \frac{(\sigma t)^2}{2})}$
    \item $X \sim \text{Exp}(\theta) \Leftrightarrow
              \mathcal{M}_X(t) = \frac{\theta}{\theta - t}$
    \item $X \sim \text{Gamma}(\alpha, \beta) \Leftrightarrow
              \mathcal{M}_X(t) = \frac{\beta^\alpha}{(\beta - t)^\alpha}$.
\end{itemize}

\subsubsection{Joint moment generating functions}

The joint MGF of $X$ and $Y$ is:
\begin{align*}
    \mathcal{M}_{X, Y}(s, t) := \mathbb{E}(e^{sX + tY}).
\end{align*}
They are such that:
\begin{align*}
    \mathcal{M}_X(s) & = \mathcal{M}_{X, Y}(s, 0)  \\
    \mathcal{M}_Y(t) & = \mathcal{M}_{X, Y}(0, t). \\
\end{align*}
We also have that $X$ and $Y$ are independent if and only if:
\begin{align*}
    \mathcal{M}_{X, Y}(s, t) = \mathcal{M}_X(s)\mathcal{M}_Y(t).
\end{align*}

\subsubsection{Independence of moment generating functions}

If $X_1, X_2, \ldots, X_n$ are independent and $Y = \sum_{i = 1}^n X_i$:
\begin{align*}
    \mathcal{M}_Y(t) = \prod_{i = 1}^n \mathcal{M}_{X_i}(t)
\end{align*}

\subsubsection{Uniqueness of moment generating functions}

The MGF uniquely defines a distribution, for two random variables $X, Y$:
\begin{align*}
    \mathcal{M}_X = \mathcal{M}_Y \Leftrightarrow X = Y.
\end{align*}

\newpage

\section{The Normal Distribution}

\subsection{Transformation and Addition of the Normal}

For $X \sim \mathcal{N}(\mu, \sigma^2)$ and $X_i \sim
    \mathcal{N}(\mu_i, \sigma_i^2)$ for $i \in \{1, 2, \ldots, n\}$,
let $\overline{X} = \frac{1}{n}(\sum_{i = 1}^n X_i)$ be the sample mean:

\begin{align*}
    aX + b                                                 & \sim \mathcal{N}(a\mu + b, a^2\sigma^2) \tag{Linear Transformation} \\
    \sum_{i = 1}^n X_i                                     & \sim
    \mathcal{N}(\sum_{i = 1}^n\mu_i, \sum_{i = 1}^n\sigma_i^2) \tag{Summed}                                                      \\
    \frac{(X - \mu)}{\sigma}                               & \sim \mathcal{N}(0, 1) \tag{Standardised}                           \\
    \overline{X}                                           & \sim \mathcal{N}(\mu, \frac{\sigma^2}{n}) \tag{Sample Mean}         \\
    \sqrt{n}\left(\frac{\overline{X} - \mu}{\sigma}\right) & \sim \mathcal{N}(0, 1). \tag{Standardised Sample Mean}
\end{align*}

\textit{It's very important to remember that multiplication and summing differ
    when dealing with the Normal (when it comes to the variance). So, if you have
    a Normal random variable $X \sim \mathcal{N}(\mu, \sigma^2)$, $2X \neq X + X$
    as $\Var(2X) = 4\sigma^2$ and $\Var(X + X) = 2\sigma^2$. This is because when
    you're multiplying, you're amplifying variation in your sample, but when you
    sum you're combining variance.}

\subsection{Independence of the Sample Mean and the Sum of Squared Difference}

For $X_i \sim \mathcal{N}(\mu_i, \sigma_i^2)$ for $i \in \{1, 2, \ldots, n\}$,
let $\overline{X} = \frac{1}{n}(\sum_{i = 1}^n X_i)$ be the sample mean.
We have that $\overline{X}$ and $\sum_{i = 1}^n(X_i - \overline{X})^2$
are independent.

\section{Sampling Distributions related to the Normal}

\subsection{The $\chi^2$ Distribution}

\subsubsection{Definition of the $\chi^2$ distribution}

We say that a random variable $X \sim \chi^2_r$ ($r$ degrees of freedom)
if:
\begin{align*}
    \mathcal{M}_X(t) = (1 - 2t)^{-r / 2}.
\end{align*}

\subsubsection{Properties of the $\chi^2$ distribution}

Let $X \sim \chi^2_r$, $Y \sim \chi^2_s$:

\begin{itemize}
    \item $X \sim \Gamma(\frac{r}{2}, \frac{1}{2})$
    \item $\mathbb{E}(X) = r$
    \item $\Var(X) = 2r$
    \item $X + Y \sim \chi^2_{r + s}$
\end{itemize}

We also have some results relating to the Normal, let $Z$ be the standard
Normal, $X_i$ for $i \in \{1, 2, \ldots, n\}$ be samples from
$\mathcal{N}(\mu, \sigma^2)$, let $\overline{X} = \frac{1}{n}
    (\sum_{i = 1}^n X_i)$ be the sample mean:

\begin{itemize}
    \item $Z^2 \sim \chi^2_1$
    \item $\sum_{i = 1}^n (\frac{X_i - \mu}{\sigma})^2 \sim \chi^2_n$
    \item $\sum_{i = 1}^n (\frac{X_i - \overline{X}}{\sigma})^2 \sim \chi^2_{n-1}$.
\end{itemize}

Finally, we also have some results relating to the Exponential and Gamma
distributions, let $X_i$ for $i \in \{1, 2, \ldots, n\}$ be samples
from $\text{Exp}(\theta)$:

\begin{itemize}
    \item $\sum_{i = 1}^n X_i \sim \Gamma(n, \theta)$
    \item $\overline{X} = \frac{1}{n} \sum_{i = 1}^n X_i \sim \Gamma(n, n\theta)$
    \item $2\theta \sum_{i = 1}^n X_i \sim \Gamma(n, 1/2) = \chi^2_{2n}$
\end{itemize}

\newpage

\subsection{The $t$ Distribution}

\subsubsection{Definition of the $t$ distribution}

For $Z \sim \mathcal{N}(0, 1)$, $X \sim \chi^2_r$ \textbf{independent} we have:
\begin{align*}
    T = \frac{Z}{\sqrt{X / r}},
\end{align*}
is distributed with a $t$ distribution with $r$ degrees of freedom ($t_r$).

\subsubsection{Properties of the $t$ distribution}

For $T \sim t_r$:

\begin{itemize}
    \item $\mathbb{E}(T) = 0$
    \item $\Var(T) = \frac{r}{r - 2}$
    \item The density of $T$ approaches $\mathcal{N}(0, 1)$ as $r\to\infty$.
\end{itemize}

\subsubsection{Samples from the Normal with $\sigma$ unknown}

For $X_i$ for $i \in \{1, 2, \ldots, n\}$ be samples from
$\mathcal{N}(\mu, \sigma^2)$, let $\overline{X} = \frac{1}{n}
    (\sum_{i = 1}^n X_i)$ be the sample mean and $S^2 = \frac{1}{n - 1}
    \sum_{i = 1}^n(X_i - \overline{X})^2$ be the sample variance. We have that:
\begin{align*}
    \sqrt{n}\left(\frac{\overline{X} - \mu}{S}\right) \sim t_{n - 1}
\end{align*}

\textit{This is \textbf{extremely} key as this allows us to perform hypothesis
    test on any Normal sample without knowing the population variance.}

\newpage

\section{Confidence Intervals}

\subsection{Definition of a Confidence Interval}

For $X_1, \ldots, X_n \sim f_{X_1, \ldots, X_n}$ with some unknown
parameter $\theta$, we have the $\alpha\%$ confidence interval
for $\theta$ is $(c_L, c_U)$ where:
\begin{align*}
    \mathbb{P}(c_L \leq \theta \leq c_U) \geq \alpha.
\end{align*}
So, $\alpha\%$ of intervals constructed in this way contain the
true value of $\theta$.

\subsection{Examples of Confidence Intervals}

\subsubsection{$\mathcal{N}(\mu, \sigma^2)$ : Confidence
    interval for $\mu$ with $\sigma^2$ known}

The $\alpha\%$ confidence interval for $\mu$ from a sample
$\{x_1, x_2, \ldots, x_n\}$ is $(c_L, c_U)$ where $\Phi^{-1}$ is the
Normal quantile function (inverse CDF) and:
\begin{align*}
    c_L & = \overline{x} - \Phi^{-1}\left(\frac{1 + \alpha}{2}\right)
    \left(\frac{\sigma}{\sqrt{n}}\right)                              \\
    c_U & = \overline{x} + \Phi^{-1}\left(\frac{1 + \alpha}{2}\right)
    \left(\frac{\sigma}{\sqrt{n}}\right).
\end{align*}

\textit{So, for a $95\%$ confidence interval:}
\begin{align*}
    c_L & = \overline{x} - \Phi^{-1}\left(0.975\right)
    \left(\frac{\sigma}{\sqrt{n}}\right)               \\
    c_U & = \overline{x} + \Phi^{-1}\left(0.975\right)
    \left(\frac{\sigma}{\sqrt{n}}\right).
\end{align*}

\subsubsection{$\mathcal{N}(\mu, \sigma^2)$ : Confidence
    interval for $\mu$ with $\sigma^2$ unknown}

The $\alpha\%$ confidence interval for $\mu$ from a sample
$\{x_1, x_2, \ldots, x_n\}$ is $(c_L, c_U)$ where $\hat\sigma^2$ is
the sample variance, $T^{-1}_{n}$ is the $t_{n}$ quantile function
(inverse CDF), and:
\begin{align*}
    c_L & = \overline{x} - T^{-1}_{n-1}\left(\frac{1 + \alpha}{2}\right)
    \left(\frac{\hat\sigma}{\sqrt{n}}\right)                             \\
    c_U & = \overline{x} + T^{-1}_{n-1}\left(\frac{1 + \alpha}{2}\right)
    \left(\frac{\hat\sigma}{\sqrt{n}}\right).
\end{align*}

\subsubsection{$\mathcal{N}(\mu, \sigma^2)$ : Confidence
    interval for $\sigma^2$ with $\mu$ unknown}

The $\alpha\%$ confidence interval for $\sigma^2$ from a sample
$\{x_1, x_2, \ldots, x_n\}$ is $(c_L, c_U)$ where $X_n^{-1}$ is the
$\chi^2_n$ quantile function (inverse CDF):
\begin{align*}
    c_L & = \frac{\sum_{i = 1}^n(x_i - \overline{x})^2}
    {X_{n-1}^{-1}\left(\frac{1 + \alpha}{2}\right)}     \\
    c_U & = \frac{\sum_{i = 1}^n(x_i - \overline{x})^2}
    {X_{n-1}^{-1}\left(\frac{1 - \alpha}{2}\right)}.
\end{align*}

\subsubsection{$U(0, \theta)$ : Confidence interval for $\theta$}

The $\alpha\%$ confidence interval for $\theta$ from a sample
$\{x_1, x_2, \ldots, x_n\}$ is $(c_L, c_U)$ where:
\begin{align*}
    c_L & = x_{(n)}\left(\frac{1 + \alpha}{2}\right)^{-\frac{1}{n}}  \\
    c_U & = x_{(n)}\left(\frac{1 - \alpha}{2}\right)^{-\frac{1}{n}}.
\end{align*}

\subsubsection{Confidence Intervals by Simulation}

For a distribution $X$ with unknown parameter $\theta$, we can
find a $\alpha\%$ confidence interval $(c_L, c_U)$ by simulation from
a sample $\{x_1, x_2, \ldots, x_n\}$:

\begin{itemize}
    \item Calculate an estimate for $\theta$ ($\hat\theta$) by using the
          sample
    \item Use the estimate as a parameter and generate $N$ samples of
          some size
    \item Calculate the estimates for $\theta$ these samples
    \item Find the values $c_L$ and $c_U$ such that $\alpha\%$ of
          the estimates are in $(c_L, c_U)$.
\end{itemize}

\newpage

\section{Notes on Hypothesis Testing: Population Means}

\subsection{Test Statistics}

Test statistics are generated with the assumption of $H_0$ being true.
This means if it's unlikely the observed value is distributed as
expected, we can reject our $H_0$ in favour of an alternative
hypothesis. This is the basis of our hypothesis tests.

\subsection{$p$-values}

$p$-values are calculated from an observed test statistic $t_{obs}$
by considering the probability that the distribution of our
test statistic $T$ is further from what is expected than our observed
value:
\begin{align*}
    p\text{-value } = \begin{cases}
        \mathbb{P}(|T| \geq |t_{obs}|) & (H_1 : \, \neq) \\
        \mathbb{P}(T \geq t_{obs})     & (H_1 : \, >)    \\
        \mathbb{P}(T \leq t_{obs})     & (H_1 : \, <).
    \end{cases}
\end{align*}
Small $p$-values indicate that it's unlikely that our
test statistic would be further from what is expected than
$t_{obs}$. This means our assumption of $H_0$ may not be
accurate.

\subsection{Error}

There's two main types of error in hypothesis testing:
\begin{itemize}
    \item \textbf{Type I error}: The probability that we
          conclude $H_0$ is false when it is in fact true
    \item \textbf{Type II error}: The probability that we
          conclude $H_0$ is true when $H_1$ is in fact true.
\end{itemize}
We can fix our Type I error to a value $\alpha$ before conducting
our test by using a critical region with a $\alpha\%$
significance level.

\vspace{\baselineskip}

We call the power of a test $P$ where:
\begin{align*}
    P = 1 - (\text{Type II error}).
\end{align*}

\newpage

\section{Hypothesis Tests: Comparision of Population Means}

\subsection{Normal Hypothesis Test}

\subsubsection{Assumptions}

For a sample $\{x_1, \ldots, x_n\}$ randomly sampled from
$\mathcal{N}(\mu, \sigma^2)$ with $\sigma^2$ known and an
expected mean $\mu_0$.

\subsubsection{Hypotheses}

We choose the hypotheses:
\begin{align*}
    H_0 : \mu & = \mu_0                                   \\
    H_1 : \mu & \, \neq \text{or} > \text{or} < \, \mu_0.
\end{align*}

\subsubsection{Test Statistic}

We have the test statistic defined by:
\begin{align*}
    T = \frac{\sqrt{n}(\overline{x} - \mu_0)}{\sigma}
    \sim \mathcal{N}(0, 1).
\end{align*}

\subsubsection{Critical Region}

For our observed test statistic $t_{obs}$, we want to generate a
critical region of values. This region $C$ changes based on our
$H_1$, let $\Phi^{-1}$ be the $\mathcal{N}(0, 1)$ quantile function
(inverse CDF), $\alpha$ be our significance level:
\begin{align*}
    H_1 : \mu \neq \mu_0 & \Rightarrow
    C = (-\infty, -c^*] \cup [c^*, \infty)
    \tag{$ c^* = \Phi^{-1}(1 - \frac{\alpha}{2}) $} \\
    H_1 : \mu > \mu_0    & \Rightarrow
    C = [c^*, \infty)
    \tag{$ c^* = \Phi^{-1}(1 - \alpha) $}           \\
    H_1 : \mu < \mu_0    & \Rightarrow
    C = (-\infty, -c^*].
    \tag{$ c^* = \Phi^{-1}(1 - \alpha) $}
\end{align*}

\newpage

\subsection{One Sample $t$-test}

\subsubsection{Assumptions}

For a sample $\{x_1, \ldots, x_n\}$ randomly sampled from
$\mathcal{N}(\mu, \sigma^2)$ with $\sigma^2$ unknown and an
expected mean $\mu_0$.

\subsubsection{Hypotheses}

We choose the hypotheses:
\begin{align*}
    H_0 : \mu & = \mu_0                                   \\
    H_1 : \mu & \, \neq \text{or} > \text{or} < \, \mu_0.
\end{align*}

\subsubsection{Test Statistic}

For $S^2$ defined as the sample variance of our sample, we have
the test statistic defined by:
\begin{align*}
    T = \frac{\sqrt{n}(\overline{x} - \mu_0)}{S} \sim t_{n-1}.
\end{align*}

\subsubsection{Critical Region}

For our observed test statistic $t_{obs}$, we want to generate a
critical region of values. This region $C$ changes based on our
$H_1$, let $T_k^{-1}$ be the $t_k$ quantile function (inverse CDF),
$\alpha$ be our significance level:
\begin{align*}
    H_1 : \mu \neq \mu_0 & \Rightarrow
    C = (-\infty, -c^*] \cup [c^*, \infty)
    \tag{$ c^* = T^{-1}_{n-1}(1 - \frac{\alpha}{2}) $} \\
    H_1 : \mu > \mu_0    & \Rightarrow
    C = [c^*, \infty)
    \tag{$ c^* = T^{-1}_{n-1}(1 - \alpha) $}           \\
    H_1 : \mu < \mu_0    & \Rightarrow
    C = (-\infty, -c^*].
    \tag{$ c^* = T^{-1}_{n-1}(1 - \alpha) $}
\end{align*}

\newpage

\subsection{Pooled Two Sample $t$-test}

\subsubsection{Assumptions}

For two \textbf{independent} samples with the \textbf{same variance}:
\begin{itemize}
    \item $\{x_1, \ldots, x_n\}$ randomly sampled from
          $\mathcal{N}(\mu_X, \sigma^2)$
    \item $\{y_1, \ldots, y_m\}$
          randomly sampled from $\mathcal{N}(\mu_Y, \sigma^2)$.
\end{itemize}

\subsubsection{Hypotheses}

We choose the hypotheses:
\begin{align*}
    H_0 : \mu_X - \mu_Y & = 0 \, \text{(the values are the same)}        \\
    H_1 : \mu_X - \mu_Y & \neq 0 \, \text{(the values are not the same)} \\
                        & > 0 \, \text{($\mu_X$ is greater)}             \\
                        & < 0 \, \text{($\mu_Y$ is greater)}.
\end{align*}

\subsubsection{Test Statistic}

We define the pooled variance $S_p^2$:
\begin{align*}
    S_p^2 = \frac{\sum_{i = 1}^n (x_i - \overline{x})^2
        + \sum_{i = 1}^m (y_i - \overline{y})^2}{n + m - 2}
    = \frac{S_{xx} + S_{yy}}{n + m - 2},
\end{align*}
and we have the test statistic $T$ defined by:
\begin{align*}
    T = \frac{\overline{x} - \overline{y}}
    {S_p\sqrt{\frac{1}{n} + \frac{1}{m}}} \sim t_{n + m - 2}.
\end{align*}

\subsubsection{Critical Region}

For our observed test statistic $t_{obs}$, we want to generate a
critical region of values. This region $C$ changes based on our
$H_1$, let $T^{-1}_k$ be the $t_k$ quantile function (inverse CDF),
$\alpha$ be our significance level:
\begin{align*}
    H_1 : \mu_X - \mu_Y \neq 0 & \Rightarrow
    C = (-\infty, -c^*] \cup [c^*, \infty)
    \tag{$ c^* = T^{-1}_{n + m - 2}(1 - \frac{\alpha}{2}) $}         \\
    H_1 : \mu_X - \mu_Y > 0    & \Rightarrow
    C = [c^*, \infty) \tag{$ c^* = T^{-1}_{n + m - 2}(1 - \alpha) $} \\
    H_1 : \mu_X - \mu_Y < 0    & \Rightarrow C = (-\infty, -c^*].
    \tag{$ c^* = T^{-1}_{n + m - 2}(1 - \alpha) $}
\end{align*}

\newpage

\subsection{Welch Two Sample $t$-test}

\subsubsection{Assumptions}

For two \textbf{independent} samples (if the variances are the
same, use the pooled test):
\begin{itemize}
    \item $\{x_1, \ldots, x_n\}$ randomly sampled from
          $\mathcal{N}(\mu_X, \sigma^2)$
    \item $\{y_1, \ldots, y_m\}$
          randomly sampled from $\mathcal{N}(\mu_Y, \sigma^2)$.
\end{itemize}

\subsubsection{Hypotheses}

We choose the hypotheses:
\begin{align*}
    H_0 : \mu_X - \mu_Y & = 0 \, \text{(the values are the same)}        \\
    H_1 : \mu_X - \mu_Y & \neq 0 \, \text{(the values are not the same)} \\
                        & > 0 \, \text{($\mu_X$ is greater)}             \\
                        & < 0 \, \text{($\mu_Y$ is greater)}.
\end{align*}

\subsubsection{Test Statistic}

Where $S_X^2, S_Y^2$ are the sample variances of $X$ and $Y$
respectively, we have the test statistic $T$ defined by:
\begin{align*}
    T = \frac{\overline{x} - \overline{y}}
    {\sqrt{\frac{S_X^2}{n} + \frac{S_Y^2}{m}}} \simeq t_\nu,
\end{align*}
where $\nu$ is defined as follows:
\begin{align*}
    \nu = \frac{(\frac{S_X^2}{n} + \frac{S_Y^2}{m})^2}
    {\frac{1}{n - 1}(\frac{S_X^2}{n})^2
        + \frac{1}{m - 1}(\frac{S_Y^2}{m})^2}
\end{align*}

\subsubsection{Critical Region}

For our observed test statistic $t_{obs}$, we want to generate a
critical region of values. This region $C$ changes based on our
$H_1$, let $T_k^{-1}$ be the $t_k$ quantile function (inverse CDF),
$\alpha$ be our significance level:
\begin{align*}
    H_1 : \mu_X - \mu_Y \neq 0 & \Rightarrow
    C = (-\infty, -c^*] \cup [c^*, \infty)
    \tag{$ c^* = T^{-1}_{\nu}(1 - \frac{\alpha}{2}) $}            \\
    H_1 : \mu_X - \mu_Y > 0    & \Rightarrow
    C = [c^*, \infty) \tag{$ c^* = T^{-1}_{\nu}(1 - \alpha) $}    \\
    H_1 : \mu_X - \mu_Y < 0    & \Rightarrow C = (-\infty, -c^*].
    \tag{$ c^* = T^{-1}_{\nu}(1 - \alpha) $}
\end{align*}

\newpage

\subsection{Paired $t$-test}

\subsubsection{Assumptions}

For two samples, $\{x_1, \ldots, x_n\}$ and $\{y_1, \ldots, y_n\}$, both
randomly sampled, we create the sample:
\begin{align*}
    \{w_1, \ldots, w_n\} \text{ where } w_i = x_i - y_i. \tag{$i \in \{1, \ldots, n\}$}
\end{align*}

We assume each $w_i$ is drawn from a $\mathcal{N}(\delta, \sigma^2)$
distribution where $\delta$ and $\sigma$ are unknown.

\subsubsection{Hypotheses}

We choose the hypotheses:
\begin{align*}
    H_0 : \delta & = 0 \, \text{(there's no underlying difference)}      \\
    H_1 : \delta & \neq 0 \, \text{(there's some underlying difference)} \\
                 & > 0 \, \text{($X$ tends to be greater than $Y$)}      \\
                 & < 0 \, \text{($Y$ tends to be greater than $X$)}.
\end{align*}

\subsubsection{Test Statistic}

Where $S_W^2$, is the sample variances of $W$, we have the
test statistic $T$ defined by:
\begin{align*}
    T = \frac{\sqrt{n} \, \overline{w}}{S_W} \sim t_{n - 1}.
\end{align*}

\subsubsection{Critical Region}

For our observed test statistic $t_{obs}$, we want to generate a
critical region of values. This region $C$ changes based on our
$H_1$, let $T^{-1}_k$ be the $t_k$ quantile function (inverse CDF),
$\alpha$ be our significance level:
\begin{align*}
    H_1 : \delta \neq 0 & \Rightarrow
    C = (-\infty, -c^*] \cup [c^*, \infty)
    \tag{$ c^* = T^{-1}_{n - 1}(1 - \frac{\alpha}{2}) $}         \\
    H_1 : \delta > 0    & \Rightarrow
    C = [c^*, \infty) \tag{$ c^* = T^{-1}_{n - 1}(1 - \alpha) $} \\
    H_1 : \delta < 0    & \Rightarrow C = (-\infty, -c^*].
    \tag{$ c^* = T^{-1}_{n - 1}(1 - \alpha) $}
\end{align*}

\newpage

\section{Linear Regression}

\subsection{Model Assumptions}

For $\{x_1, \ldots, x_n\}$ values of observed of a predictor
variable $X$ with $y_i (i \in \{1, \ldots, n\})$ the values
of the response variable $Y$:
\begin{align*}
    y_i = \alpha + \beta x_i + e_i,
\end{align*}
where $\alpha$ and $\beta$ are unknown and each $e_i$ represents the
error.

\subsection{Errors}

For the error values $\{e_1, \ldots, e_n\}$, we assume that:
\begin{itemize}
    \item $\mathbb{E}(e_i) = 0$
    \item $\Var(e_i) = \sigma^2$ (unknown)
    \item $\Cov(e_i, e_j) = 0$ for $i \neq j$ (uncorrelated error).
\end{itemize}
As a consequence, we have that:
\begin{itemize}
    \item $\mathbb{E}(y_i) = \alpha + \beta x_i$
    \item $\Var(y_i) = \sigma^2$
    \item $\Cov(y_i, y_j) = 0$ for $i \neq j$ (uncorrelated responses).
\end{itemize}

\subsection{Summary Statistics}

We define a few summary statistics used in our calculations:
\begin{itemize}
    \item $\overline{x} = \frac{1}{n}\sum_{i = 1}^n x_i$
    \item $\overline{y} = \frac{1}{n}\sum_{i = 1}^n y_i$
    \item $S_{xx} = \sum_{i=i}^n (x_i)^2 - n\overline{x}^2$
    \item $S_{xy} = \sum_{i=1}^n (x_i y_i) - n\overline{x}\overline{y}$
    \item $S_{yy} = \sum_{i=i}^n (y_i)^2 - n\overline{y}^2.$
\end{itemize}

\newpage

\subsection{Finding $\hat\alpha$ and $\hat\beta$}

By minimising the squared error between our estimated and observed
values, we develop least squares estimates for $\alpha$ and $\beta$:
\begin{itemize}
    \item $\hat\beta = \frac{S_{xy}}{S_{xx}}$
    \item $\hat\alpha = \overline{y} - \hat\beta\overline{x}$
\end{itemize}

\subsection{Residual Sum of Squares and $\hat\sigma^2$}

We defines the residual sum of squares $RSS$ as the sum of
all the errors squared, an alternate formula for this is:
\begin{align*}
    RSS = S_{yy} - \frac{S_{xy}^2}{S_{xx}}.
\end{align*}
Then, we can estimate $\sigma^2$ by:
\begin{align*}
    \hat\sigma^2 = \frac{RSS}{n - 2}
    = \frac{S_{yy} - \frac{S_{xy}^2}{S_{xx}}}{n - 2}.
\end{align*}

\section{Notes on Hypothesis Testing: Regression}

\subsection{Assumption of Normality}

To perform linear regression hypothesis tests, we use an assumption
of normality. We assume the errors $e_i$ are independent and
identically distributed $\mathcal{N}(0, \sigma^2)$.

\subsection{The Distribution of $\hat\alpha$,
    $\hat\beta$, and $\hat\sigma^2$}

We have the distribution of $\hat\alpha$ and
$\hat\beta$:
\begin{align*}
    \hat\beta  & \sim \mathcal{N}\left(\beta,
    \frac{\sigma^2}{S_{xx}}\right)             \\
    \hat\alpha & \sim \mathcal{N}\left(\alpha,
    \frac{\Var(\hat\beta) \sum_{i = 1}^n x_i^2}{n}\right).
\end{align*}

\newpage

We write the following for the sample variance of $\alpha$ and $\beta$:
\begin{align*}
    s_{\hat\beta}^2  & = \frac{\hat\sigma^2}{S_{xx}}                   \\
    s_{\hat\alpha}^2 & = \frac{s_{\hat\beta}^2 \sum_{i=1}^n x_i^2}{n}.
\end{align*}
In terms of $t$ and $\chi^2$ distributions:
\begin{align*}
    \frac{(n - 2)\hat\sigma^2}{\sigma^2}       & = \frac{RSS}{\sigma^2}
    \sim \chi^2_{n - 2}                                                 \\
    \frac{\hat\alpha - \alpha}{s_{\hat\alpha}} & \sim t_{n - 2}         \\
    \frac{\hat\beta - \beta}{s_{\hat\beta}}    & \sim t_{n - 2}.
\end{align*}
In practice, we substitute $\alpha$, $\beta$, and $\sigma^2$ for
some hypothesised value. For example:
\begin{align*}
    H_0 : \beta = 0 \Rightarrow
    \frac{\hat\beta}{s_{\hat\beta}} \sim t_{n - 2}.
\end{align*}

\section{Confidence Intervals for $\alpha$ and $\beta$}

\subsection{Confidence Interval for $\alpha$}

The $\gamma\%$ confidence interval for $\alpha$ from a sample
$\{x_1, x_2, \ldots, x_n\}$ with response set
$\{y_1, y_2, \ldots, y_n\}$ is $(c_L, c_U)$ where $T^{-1}_n$ is the
$t_n$ quantile function (inverse CDF) and:
\begin{align*}
    c_L & = \hat\alpha - s_{\hat\alpha}
    T^{-1}_{n - 2}\left(\frac{1 + \gamma}{2}\right) \\
    c_U & = \hat\alpha + s_{\hat\alpha}
    T^{-1}_{n - 2}\left(\frac{1 + \gamma}{2}\right).
\end{align*}

\subsection{Confidence Interval for $\beta$}

The $\gamma\%$ confidence interval for $\beta$ from a sample
$\{x_1, x_2, \ldots, x_n\}$ with response set
$\{y_1, y_2, \ldots, y_n\}$ is $(c_L, c_U)$ where $T^{-1}_n$ is the
$t_n$ quantile function (inverse CDF) and:
\begin{align*}
    c_L & = \hat\beta - s_{\hat\beta}
    T^{-1}_{n - 2}\left(\frac{1 + \gamma}{2}\right) \\
    c_U & = \hat\beta + s_{\hat\beta}
    T^{-1}_{n - 2}\left(\frac{1 + \gamma}{2}\right).
\end{align*}

\section{Hypothesis Tests: Linear Regression}

\subsection{Hypothesis Test for $\beta$}

\subsubsection{Assumptions}

For a sample $\{x_1, \ldots, x_n\}$ randomly sampled with response
set $\{y_1, \ldots, y_n\}$ with Normal errors following a
$\mathcal{N}(0, \sigma^2)$ and an expected value $\beta_0$.

\subsubsection{Hypotheses}

We choose the hypotheses:
\begin{align*}
    H_0 : \beta & = \beta_0                                   \\
    H_1 : \beta & \, \neq \text{or} > \text{or} < \, \beta_0.
\end{align*}

\subsubsection{Test Statistic}

For $s_{\hat\beta}^2$ defined as follows:
\begin{align*}
    s_{\hat\beta}^2 & = \frac{\hat\sigma^2}{S_{xx}},
\end{align*}
we have the test statistic defined by:
\begin{align*}
    T = \frac{(\hat\beta - \beta_0)}{s_{\hat\beta}}
    \sim t_{n - 2}.
\end{align*}

\subsubsection{Critical Region}

For our observed test statistic $t_{obs}$, we want to generate a
critical region of values. This region $C$ changes based on our
$H_1$, let $T^{-1}_n$ be the $t_n$ quantile function
(inverse CDF), $\gamma$ be our significance level:
\begin{align*}
    H_1 : \beta \neq \beta_0 & \Rightarrow
    C = (-\infty, -c^*] \cup [c^*, \infty)
    \tag{$ c^* = T_{n-2}^{-1}(1 - \frac{\gamma}{2}) $} \\
    H_1 : \beta > \beta_0    & \Rightarrow
    C = [c^*, \infty)
    \tag{$ c^* = T_{n-2}^{-1}(1 - \gamma) $}           \\
    H_1 : \beta < \beta_0    & \Rightarrow
    C = (-\infty, -c^*].
    \tag{$ c^* = T_{n-2}^{-1}(1 - \gamma) $}
\end{align*}

\newpage

\subsection{Hypothesis Test for $\alpha$}

\subsubsection{Assumptions}

For a sample $\{x_1, \ldots, x_n\}$ randomly sampled with response
set $\{y_1, \ldots, y_n\}$ with Normal errors following a
$\mathcal{N}(0, \sigma^2)$ and an expected value $\alpha_0$.

\subsubsection{Hypotheses}

We choose the hypotheses:
\begin{align*}
    H_0 : \alpha & = \alpha_0                                   \\
    H_1 : \alpha & \, \neq \text{or} > \text{or} < \, \alpha_0.
\end{align*}

\subsubsection{Test Statistic}

For $s_{\hat\alpha}^2$ defined as follows:
\begin{align*}
    s_{\hat\alpha}^2 & = s_{\hat\beta}^2
    \left(\frac{\sum_{i = 1}^n x_i^2}{n}\right),
\end{align*}
we have the test statistic defined by:
\begin{align*}
    T = \frac{(\hat\alpha - \alpha_0)}{s_{\hat\alpha}}
    \sim t_{n - 2}.
\end{align*}

\subsubsection{Critical Region}

For our observed test statistic $t_{obs}$, we want to generate a
critical region of values. This region $C$ changes based on our
$H_1$, let $T^{-1}_n$ be the $t_n$ quantile function
(inverse CDF), $\gamma$ be our significance level:
\begin{align*}
    H_1 : \alpha \neq \alpha_0 & \Rightarrow
    C = (-\infty, -c^*] \cup [c^*, \infty)
    \tag{$ c^* = T_{n-2}^{-1}(1 - \frac{\gamma}{2}) $} \\
    H_1 : \alpha > \alpha_0    & \Rightarrow
    C = [c^*, \infty)
    \tag{$ c^* = T_{n-2}^{-1}(1 - \gamma) $}           \\
    H_1 : \alpha < \alpha_0    & \Rightarrow
    C = (-\infty, -c^*].
    \tag{$ c^* = T_{n-2}^{-1}(1 - \gamma) $}
\end{align*}

\newpage
\section{A List of Important Formulae}
Let $X = \{x_1, \ldots, x_n\}$ be a random sample:

\subsection{Measures of Central Tendency}

\subsubsection{Sample Mean}
\begin{gather*}
    \overline{X} = \frac{1}{n}\sum_{i = 1}^n x_i.
\end{gather*}

\subsubsection{Sample Median}
\begin{gather*}
    m_X = \begin{cases}
        \frac{x_{(m)} + x_{(m + 1)}}{2} & \text{for } n = 2m      \\
        x_{(m + 1)}                     & \text{for } n = 2m + 1.
    \end{cases}
\end{gather*}

\subsection{Measures of Spread}

\subsubsection{Sample Variance}
\begin{gather*}
    s_X^2 = \frac{\sum_{i = 1}^n (x_i - \overline{x})^2}{n - 1}.
\end{gather*}

\subsubsection{Hinges}
\begin{align*}
    H_1 & = \text{ median of \{data values } \leq \text{ the median\}}  \\
    H_3 & = \text{ median of \{data values } \geq \text{ the median\}}.
\end{align*}

\subsubsection{Quartiles}
\begin{align*}
    Q_1 & = x_{(k_1)}, \, k_1 = \frac{ (n + 1)}{4} \tag{interpolated} \\
    Q_3 & = x_{(k_3)}, \, k_3 = \frac{3(n + 1)}{4} \tag{interpolated}
\end{align*}

\subsubsection{Outliers}
\begin{gather*}
    O(X) = \left\{x \in X \text{ such that }
    |x - \overline{X}| > \frac{3}{2}(H_3 - H_1)\right\}
\end{gather*}

\subsection{Estimators}

\subsubsection{Maximum Likelihood}
For each $x$ in $X$ independent and identically distributed:
\begin{gather*}
    \frac{\partial}{\partial\theta}\ell(\theta) =
    \sum_{i = 1}^n\left[\frac{\partial}{\partial\theta}\ln{(f_X(x_i))}\right].
\end{gather*}

\subsubsection{Performance of Estimators}
\begin{align*}
    \Bias(\hat\theta) & = \mathbb{E}(\hat\theta) - \theta         \\
    \Mse(\hat\theta)  & = \mathbb{E}((\hat\theta - \theta)^2)     \\
                      & = \Var(\hat\theta) + \Bias(\hat\theta)^2.
\end{align*}

\subsection{Distributions}

\subsubsection{Distributions related to the normal}
If for $X_i$ in $X$ we have $X_i \sim \mathcal{N}(\mu, \sigma^2)$:
\begin{gather*}
    \overline{X} = \frac{1}{n} \sum_{i = 1}^n X_i
    \sim \mathcal{N}(\mu, \sigma^2 / n) \\
    S_X^2 = \frac{\sum_{i = 1}^n (X_i - \overline{X})^2}{n - 1}
    \sim \frac{\sigma^2}{n - 1} \chi^2_{n - 1}.
\end{gather*}

\subsubsection{$\chi^2$ distribution}
\begin{gather*}
    \mathcal{M}_X(t) = (1 - 2t)^{-r / 2} \Longleftrightarrow
    X \sim \chi^2_r.
\end{gather*}

\subsubsection{$t$-distribution}
Let $Z \sim \mathcal{N}(0, 1)$, $X \sim \chi_r^2$ be independent, for:
\begin{gather*}
    T = \frac{Z}{\sqrt{X / r}} \sim t_r.
\end{gather*}

\subsection{Sum of Squared Difference}
\begin{align*}
    S_{xx} & = \sum_{i=i}^n (x_i)^2 - n\overline{x}^2                     \\
    S_{xy} & = \sum_{i=1}^n (x_{(i)} y_{(i)}) - n\overline{x}\overline{y} \\
    S_{yy} & = \sum_{i=i}^n (y_i)^2 - n\overline{y}^2.
\end{align*}

\subsection{Linear Regression}
For $\{y_1, \ldots, y_n\}$ a random sample predicted by $X$:

\subsubsection{$\hat\alpha$ and $\hat\beta$}
\begin{gather*}
    \hat\beta = \frac{S_{xy}}{S_{xx}} \\
    \hat\alpha = \overline{y} - \hat\beta\overline{x}.
\end{gather*}

\subsubsection{Sum of squared errors}
\begin{gather*}
    RSS = S_{yy} - \frac{S_{xy}^2}{S_{xx}}.
\end{gather*}

\subsubsection{Estimated variance of the errors}
\begin{gather*}
    \hat\sigma^2 = \frac{RSS}{n - 2}.
\end{gather*}

\subsubsection{The distribution of $\hat\alpha$, $\hat\beta$
    and, $\hat\sigma^2$}
\begin{align*}
    \hat\beta    & \sim \mathcal{N}\left(\beta,
    \frac{\sigma^2}{S_{xx}}\right)                            \\
    \hat\alpha   & \sim \mathcal{N}\left(\alpha,
    \frac{\Var(\hat\beta) \sum_{i = 1}^n x_i^2}{n}\right)     \\
    \hat\sigma^2 & \sim \frac{\sigma^2}{n - 2}\chi^2_{n - 2}.
\end{align*}

\subsubsection{Sample variance of $\hat\alpha$ and $\hat\beta$}
\begin{align*}
    s_{\hat\beta}^2  & = \frac{\hat\sigma^2}{S_{xx}}                   \\
    s_{\hat\alpha}^2 & = \frac{s_{\hat\beta}^2 \sum_{i=1}^n x_i^2}{n}.
\end{align*}

\subsection{Confidence Intervals}
For an $\alpha\%$ confidence interval:

\subsubsection{$\mathcal{N}(\mu, \sigma^2)$ : Confidence
    interval for $\mu$ with $\sigma^2$ known}
Let $\Phi^{-1}$ be the Normal quantile function (inverse CDF):
\begin{align*}
    C = \overline{x} \pm \Phi^{-1}\left(\frac{1 + \alpha}{2}\right)
    \left(\frac{\sigma}{\sqrt{n}}\right).
\end{align*}

\subsubsection{$\mathcal{N}(\mu, \sigma^2)$ : Confidence
    interval for $\mu$ with $\sigma^2$ unknown}
Let $T^{-1}_{n}$ be the $t_{n}$ quantile function (inverse CDF),
$\hat\sigma^2$ be the sample variance:
\begin{align*}
    C = \overline{x} \pm T^{-1}_{n-1}\left(\frac{1 + \alpha}{2}\right)
    \left(\frac{\hat\sigma}{\sqrt{n}}\right).
\end{align*}

\subsubsection{$\mathcal{N}(\mu, \sigma^2)$ : Confidence
    interval for $\sigma^2$ with $\mu$ unknown}
Let $X_n^{-1}$ be the $\chi^2_n$ quantile function (inverse CDF):
\begin{align*}
    C = \frac{\sum_{i = 1}^n(x_i - \overline{x})^2}
    {X_{n-1}^{-1}\left(\frac{1 \pm \alpha}{2}\right)}.
\end{align*}

\subsubsection{$U(0, \theta)$ : Confidence interval for $\theta$}
\begin{align*}
    C = x_{(n)}\left(\frac{1 \pm \alpha}{2}\right)^{-\frac{1}{n}}.
\end{align*}

\subsubsection{Confidence interval for $\alpha$}
Let $T^{-1}_n$ be the $t_n$ quantile function (inverse CDF),
the $\gamma\%$ confidence interval is:
\begin{align*}
    C = \hat\alpha \pm s_{\hat\alpha}
    T^{-1}_{n - 2}\left(\frac{1 + \gamma}{2}\right).
\end{align*}

\subsubsection{Confidence interval for $\beta$}
Let $T^{-1}_n$ be the $t_n$ quantile function (inverse CDF):
\begin{align*}
    C = \hat\beta \pm s_{\hat\beta}
    T^{-1}_{n - 2}\left(\frac{1 + \alpha}{2}\right).
\end{align*}

\subsection{Test Statistics}

\subsubsection{Normal Hypothesis Test}
\begin{gather*}
    T = \frac{\sqrt{n}(\overline{x} - \mu_0)}{\sigma}
    \sim \mathcal{N}(0, 1).
\end{gather*}

\subsubsection{One Sample $t$-test}
\begin{gather*}
    T = \frac{\sqrt{n}(\overline{x} - \mu_0)}{S} \sim t_{n-1}.
\end{gather*}

\subsubsection{Pooled Two Sample $t$-test}
\begin{gather*}
    S_p^2 = \frac{\sum_{i = 1}^n (x_i - \overline{x})^2
        + \sum_{i = 1}^m (y_i - \overline{y})^2}{n + m - 2}
    = \frac{S_{xx} + S_{yy}}{n + m - 2} \\ \\
    T = \frac{\overline{x} - \overline{y}}
    {S_p\sqrt{\frac{1}{n} + \frac{1}{m}}} \sim t_{n + m - 2}.
\end{gather*}

\subsubsection{Welch Two Sample $t$-test}
\begin{gather*}
    \nu = \frac{(\frac{S_X^2}{n} + \frac{S_Y^2}{m})^2}
    {\frac{1}{n - 1}(\frac{S_X^2}{n})^2
        + \frac{1}{m - 1}(\frac{S_Y^2}{m})^2} \\ \\
    T = \frac{\overline{x} - \overline{y}}
    {\sqrt{\frac{S_X^2}{n} + \frac{S_Y^2}{m}}} \simeq t_\nu.
\end{gather*}

\subsubsection{Paired $t$-test}
\begin{gather*}
    T = \frac{\sqrt{n} \, \overline{w}}{S_W} \sim t_{n - 1}.
\end{gather*}

\subsubsection{Test for $\alpha$}
\begin{gather*}
    T = \frac{(\hat\alpha - \alpha)}{s_{\hat\alpha}}
    \sim t_{n - 2}.
\end{gather*}

\subsubsection{Test for $\beta$}
\begin{gather*}
    T = \frac{(\hat\beta - \beta)}{s_{\hat\beta}}
    \sim t_{n - 2}.
\end{gather*}

\end{document}