\documentclass[a4paper, 12pt, twoside]{article}
\usepackage[left = 3cm, right = 3cm]{geometry}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multicol}
\usepackage{pgfplots}
\usepackage{pgfplotstable}
\usepackage{listings}
\usepackage{xcolor}
\usepackage{color}

\pgfplotsset{compat=1.5.1}

\lstset{frame=none,
  language=C++,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{orange},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=2
}

\begin{document}

\title{Data Structures and Algorithms Notes}
\date{}
\author{\textit{paraphrased by} Tyler Wright}
\maketitle

\vfill

\textit{An important note, these notes are absolutely \textbf{NOT}
  guaranteed to be correct, representative of the course, or rigorous.
  Any result of this is not the author's fault.}

\newpage

\section{Data Structures}

\subsection{Stacks}

A stack is a list of variables. It supports three operations:

\begin{center}
  \begin{tabular}{ || c | p{6.5cm} | c || }
    \hline
    Name & Description & Worst case runtime \\
    \hline
    \texttt{create()} & Creates a new stack & $O(1)$ \\
    \hline
    \texttt{push(x)} & Adds \texttt{x} to the end of the stack & $O(1)$ \\
    \hline
    \texttt{pop()} & Removes and returns the last element of the stack & $O(1)$ \\
    \hline
  \end{tabular}
\end{center}

\subsection{Queues}

A queue is a list of variables. It supports three operations:

\begin{center}
  \begin{tabular}{ || c | p{6.5cm} | c || }
    \hline
    Name & Description & Worst case runtime \\
    \hline
    \texttt{create()} & Creates a new queue & $O(1)$ \\
    \hline
    \texttt{add(x)} & Adds \texttt{x} to the end of the queue & $O(1)$ \\
    \hline
    \texttt{serve()} & Removes and returns the first element of the queue & $O(1)$ \\
    \hline
  \end{tabular}
\end{center}

\subsection{Linked List}

A linked list is a list of variables represented by nodes which
point to the next and previous element in the list (null if one does
not exist). 
It supports four operations:

\begin{center}
  \begin{tabular}{ || c | p{6.5cm} | c || }
    \hline
    Name & Description & Worst case runtime \\
    \hline
    \texttt{create()} & Creates a new linked list & $O(1)$ \\
    \hline
    \texttt{insert(x, i)} & Inserts \texttt{x} after node \texttt{i} & $O(1)$ \\
    \hline
    \texttt{delete(i)} & Removes node \texttt{i} & $O(1)$ \\
    \hline
    \texttt{lookup(i)} & Returns node \texttt{i} & $O(1)$ \\
    \hline
  \end{tabular}
\end{center}

\subsection{Arrays}

An array is a list of variables of fixed length. 
It supports three operations:

\begin{center}
  \begin{tabular}{ || c | p{6.5cm} | c || }
    \hline
    Name & Description & Worst case runtime \\
    \hline
    \texttt{create(n)} & Creates a new array of size \texttt{n} & $O(1)$ \\
    \hline
    \texttt{update(x, i)} & Overwrites the data at position \texttt{i} with \texttt{x} & $O(1)$ \\
    \hline
    \texttt{lookup(i)} & Returns the value at \texttt{i} & $O(1)$ \\
    \hline
  \end{tabular}
\end{center}

\subsection{Hash Tables}

A hash table is an array of linked lists storing key-value pairs. 
We use a \textbf{hash function} to map data to a linked list. As we 
are using linked lists, if multiple keys map to the same index, 
we can just add them to the list - and when looking up data, we 
can find the right list with the hash function and then match our key.
\\[\baselineskip]
It supports four operations:

\begin{center}
  \begin{tabular}{ || c | p{7.5cm} | c || }
    \hline
    Name & Description & Average runtime \\
    \hline
    \texttt{create(n)} & Creates a \texttt{n} sized array
    of linked lists and chooses a hash function \texttt{h} & $O(1)$ \\
    \hline
    \texttt{insert(k, v)} & Inserts the pair (\texttt{k, v}),
    if $\frac{\texttt{n}}{2}$ pairs are stored, we create a hash
    table of double the size and copy the contents into it & $O(1)$ \\
    \hline
    \texttt{delete(k)} & Deletes the pair corresponding to the key \texttt{k} & $O(1)$ \\
    \hline
    \texttt{lookup(k)} & Returns the pair corresponding to the key \texttt{k} & $O(1)$ \\
    \hline
  \end{tabular}
\end{center}

\subsubsection{Markov's Inequality}

For $X \geq 0$ a random variable with mean $\mu$, for all
$t$ in $\mathbb{R}_{\geq 0}$: \begin{gather*}
  \mathbb{P}(X \geq t) \leq \frac{\mu}{t}.
\end{gather*} So, if $X$ is the expected time it takes for an 
algorithm to terminate, we can say how likely it is for an algorithm
to terminate based on our prediction.

\subsection{Binary Heaps}

Binary heaps are rooted binary trees where each level is full except
possibly the last (which is filled from left to right). The
elements of the tree are ordered according to a 
\textbf{heap property}. These have the following properties: \begin{itemize}
  \item For a heap of size $n$, the height of the heap is $\log_2(n)$
  \item For an index $i$: \begin{itemize}
    \item The parent has index $\left\lfloor \dfrac{i}{2} \right\rfloor$
    \item The left child has index $2i$
    \item The right child has index $2i + 1$
  \end{itemize}
\end{itemize}

\newpage

\subsection{Priority Queues}

A priority queue is a set of distinct elements with associated value
called the key. We can use a binary heap as a priority queue with the
elements as the keys and the heap property that the parents are
less than or equal to the children. This supports the following:
\begin{center}
  \begin{tabular}{ || c | p{7.5cm} | c || }
    \hline
    Name & Description & Runtime \\
    \hline
    \texttt{insert(x, k)} & Inserts \texttt{x} with key \texttt{k}
    & $O(\log_2(n))$ \\
    \hline
    \texttt{decreaseKey(x, d)} & Decreases the key of \texttt{x} to \texttt{d}
    & $O(\log_2(n))$ \\
    \hline
    \texttt{extractMin()} & Removes and returns the \texttt{x} in the queue
    with the smallest key & $O(\log_2(n))$ \\
    \hline
  \end{tabular}
\end{center}

\subsection{Disjoint Set}

Stores a collection of disjoint sets where each set has elements
$1, 2, \ldots n$ for some natural $n$. This supports the following:
\begin{center}
  \begin{tabular}{ || c | p{7.5cm} | c || }
    \hline
    Name & Description & Runtime \\
    \hline
    \texttt{makeSet(x)} & Creates a new set containing only \texttt{x}
    this fails if \texttt{x} is already in a set
    & $O(1)$ \\
    \hline
    \texttt{union(x, y)} & Merges the sets containing 
    \texttt{x} and \texttt{y}
    & $O(\log_2(n))$ \\
    \hline
    \texttt{findSet(x)} & Finds the identifier of the set containing
    \texttt{x} (the identifier is an element of the set)
    & $O(\log_2(n))$ \\
    \hline
  \end{tabular}
\end{center} This is stored as an array size $n$ where each cell
is empty or points to the identifier of the set it was originally
added to.
\\[\baselineskip]
\textit{So, adding $3$ to $\{7\}$ would make $3$ always point to 
$7$. But creating a set with $3$, will make it point to itself.}

\subsection{Dynamic Search Structures}

This structure stores a set of elements, each with a unique key.
This supports the following:
\begin{center}
  \begin{tabular}{ || c | p{7.5cm} | c || }
    \hline
    Name & Description & Runtime \\
    \hline
    \texttt{insert(x, k)} & Inserts \texttt{x}
    with key \texttt{k}
    & $O(\log_2(n))$ \\
    \hline
    \texttt{find(k)} & Returns the element with 
    unique key \texttt{k}
    & $O(\log_2(n))$ \\
    \hline
    \texttt{delete(k)} & Deletes the element with unique key
    \texttt{k}
    & $O(\log_2(n))$ \\
    \hline
    \texttt{predecessor(k)} & Returns the element with unique key
    \texttt{n} such that \texttt{n} $<$ \texttt{k}
    & $O(\log_2(n))$ \\
    \hline
    \texttt{rangeFind(a, b)} & Returns the elements with unique key
    \texttt{k} such that \texttt{a} $\leq$ \texttt{k} $\leq$ \texttt{b}
    & $O(\log_2(n))$ \\
    \hline
  \end{tabular}
\end{center}

\vfill

\section{Graph Theory}

\subsection{Definition of a Graph}

A graph is a pair of sets $G = (V, E)$, where $V$ is a set of 
vertices (or nodes) and $E$ is a set of edges (or arcs).

\subsection{Definition of an Edge}

An edge of a graph $G = (V, E)$ is $e = \{u, v\}$ in $E$ where $u$,
$v$ are vertices in $V$.

\subsection{Definition of a Neighbourhood}

For a graph $G = (V, E)$ with $v$ in $V$, the neighbourhood
of $v$ is the set $V' \subseteq V$ of vertices connected to
$v$ by an edge in $E$.
\\[\baselineskip]
The neighbourhood of $v$ is denoted by $N(v)$.
\\[\baselineskip]
The neighbourhood of a set of vertices is the union of
the neighbourhoods of each vertex.

\subsection{Definition of Degree}

For a graph $G = (V, E)$ with $v$ in $V$, the degree of $v$
is the size of its neighbourhood.
\\[\baselineskip]
The degree of $v$ is denoted by $d(v)$.

\subsection{The Handshake Lemma}

For a graph $G = (V, E)$, we have that: \begin{gather*}
  |E| = \frac{\sum_{v \in V} d(v)}{2}.
\end{gather*} \textit{This is because each edge visits two vertices,
so by counting the degree of each vertex we count each edge exactly
twice.}

\subsection{$k$-regular Graphs}

For a graph $G = (V, E)$, we have that $G$ is $k$-regular for some
$k$ in $\mathbb{Z}_{>0}$ if for all $v$ in $V$, we have: \begin{gather*}
  d(v) = k.
\end{gather*} We cannot have a $k$-regular graph where $k$ is odd
and $|V|$ is odd by the Handshake Lemma.

\subsection{Isomorphic Graphs}

Graphs $G_1 = (V_1, E_1)$ and $G_2 = (V_2, E_2)$ are called 
isomorphic if there exists a bijection $f : V_1 \to V_2$ such
that: \begin{gather*}
  \{u, v\} \in E_1 \Longleftrightarrow \{f(u), f(v)\} \in E_2.
\end{gather*} This relationship is denoted by $G_1 \cong G_2$.

\subsection{Definition of a Subgraph}

A graph $G' = (V', E')$ is a subgraph of $G = (V, E)$ if
$V' \subseteq V$ and $E' \subseteq E$.

\subsection{Definition of an Induced Subgraph}

An induced subgraph generated from $G = (V, E)$ by $V' \subseteq V$
is the graph $G' = (V', E')$ where: \begin{gather*} 
  E' = \{\{u, v\} \in E \text{ such that } u, v \in V'\}.
\end{gather*} \textit{Essentially, you generate an induced
subgraph from a subset of the vertices of a graph by selecting
edges that join vertices in the subset.}

\subsection{Walks}

\subsubsection{Definition of a walk}

A walk in a graph $G = (V, E)$ is a set of vertices in $V$ connected
by edges in $E$. The length of the walk is the number of edges
traversed in the walk.

\subsubsection{Definition of a path}

A path is a walk where no vertices are repeated.

\subsubsection{Definition of an Euler walk}

An Euler walk is a walk such that every edge is traversed exactly
once. Thus, for a graph $G = (V, E)$, the length is $|E|$.

\subsubsection{Conditions for an Euler walk}

For an Euler walk to be possible on a given graph,
all vertices must have an even degree \textbf{or} exactly
two vertices have odd degree. 
\\[\baselineskip]
If all vertices have even degree we 
have that the Euler walk is a cycle, if exactly two vertices have
odd degree then we have that these vertices are the start and end
points of our Euler walk. 

\subsection{Definition of a Connected Graph}

A connected graph is a graph where for each pair of vertices,
there is a path connecting them.

\subsection{Definition of a Component}

A component of a graph $G$ is a maximal connected 
induced subgraph of $G$. This means an induced subgraph of $G$
that is connected but is not longer connected if a vertex is
removed.

\subsection{Digraphs}

\subsubsection{Definition of a digraph}

A digraph (or directed graph) is a graph where each of the edges
has a direction. This direction means the edge can only be traversed
in a single direction.

\subsubsection{The Directed Handshake Lemma}

For a digraph $G = (V, E)$, we have that: \begin{gather*}
  \sum_{v \in V} d^-(v) = \sum_{v \in V} d^+(v) = |E|.
\end{gather*} \textit{This is because if we consider the 'tail' of an
edge (the vertex it leaves), each edge has exactly one tail.}

\subsubsection{Definition of a strongly connected digraph}

A digraph $G = (V, E)$ is strongly connected if for each $u, v$
in $E$, there exists a path from $u$ to $v$ \textbf{and} 
from $v$ to $u$.

\subsubsection{Definition of a weakly connected digraph}

A digraph $G = (V, E)$ is weakly connected if for each $u, v$
in $E$, there exists a path from $u$ to $v$ \textbf{or} 
from $v$ to $u$.

\subsubsection{Definition of components of digraphs}

A strong component of a digraph is the maximal \textit{strongly}
connected induced subgraph.
\\[\baselineskip]
A weak component of a digraph is the maximal \textit{weakly}
connected induced subgraph.
\\[\baselineskip]
\textit{So, these are induced subgraphs that are strongly/weakly
connected but are no longer strongly/weakly connected once a
vertex is removed.}

\subsubsection{Definition of neighbourhoods in digraphs}

The neighbourhood of a vertex in a digraph can be considered by
looking at the edges \textit{from} the vertex and the edges
\textit{to} the vertex.
\\[\baselineskip]
The in-neighbourhood of a vertex $v$ are the edges that enter $v$.
The out-neighbourhood of a vertex $v$ are the edges that exit $v$.
These are denoted by $N^-(v)$ and $N^+(v)$ respectively.

\subsubsection{Definition of degrees in digraphs}

For a vertex $v$, the in-degree of the vertex $d^-(v)$ is the size of
the in-neighbourhood and the out-degree of the vertex $d^+(v)$
is the size of the out-neighbourhood.
\\[\baselineskip]
It can be seen that the degree of a given vertex is the sum of
its in and out degree (in a digraph).

\subsubsection{Conditions for an Euler walk in a digraph}

For an Euler walk to be possible on a given digraph, we have
two cases, either:\begin{itemize}
  \item the digraph is strongly connected and every vertex
  has equal in and out degrees, or
  \item one vertex has an in-degree one greater than its out-degree,
  another has an out-degree one greater than its in-degree, and all
  remaining vertices have equal in and out degrees.
\end{itemize}
In the first case we have that the Euler walk is a cycle, 
in the second we have that the special vertices are the start and end
points of our Euler walk. 

\subsubsection{Cycles}

\subsubsection{Definition of a cycle}

A cycle is a walk where the first and last vertices are the same
and each vertex appears at most once (barring the first and last
vertex).

\subsubsection{Definition of a Hamiltonian cycle}

A Hamiltonian cycle is a cycle where each vertex is visited.

\subsubsection{Conditions for a Hamiltonian cycle}

Whilst the conditions necessary for a Hamiltonian cycle in general
are unknown, by Dirac's theorem, we know that for a graph with
$n$ vertices, if every vertex has degree $\frac{n}{2}$ or greater
then a Hamiltonian cycle exists.

\subsection{Trees}

\subsubsection{Definition of a forest}

A forest is a graph with no cycles.

\subsubsection{Definition of a tree}

A tree is a connected forest (or a connected graph with no cycles).

\subsubsection{Path uniqueness of trees}

For a tree $T = (V, E)$, we have that for any $u, v$ in $V$, there
exists a unique path from $u$ to $v$.
\\[\baselineskip]
\textit{To prove this, suppose there are two unique paths between
$u$ and $v$. These paths must diverge and if we connect them, they
form a cycle which contradicts the definition of a tree.}

\subsubsection{The magnitude of edges in trees}

For a tree $T = (V, E)$, we have that $|E| = |V| - 1$.

\subsubsection{Rooted trees}

For a tree $T = (V, E)$, we can root $T$ with some $r$ in $V$.
For $v$ in $V\setminus{r}$, we define $P_v$ to be the path from 
$r$ to $v$, we then direct the edges from $r$ to $v$ for each $P_v$.
\\[\baselineskip]
For $u, v$ in $V\setminus\{r\}$, we say that: \begin{itemize}
  \item $u$ is an \textbf{ancestor} of $v$ if $u$ lies on $P_v$
  \item $u$ is the \textbf{parent} of $v$ if $u$ is in the
  in-neighbourhood of $v$
  \item $v$ is a \textbf{leaf} if it has degree $1$
  \item $L_0 = \{r\}$ and $L_n = \{v : |P_v| = n\}$ are the 
  \textbf{levels} of $T$
  \item The \textbf{depth} of a tree is the greatest $n$ where
  $L_n$ is non-empty.
\end{itemize} 

\subsubsection{Lower bound on the amount of leaves in a tree}

For a tree with $T = (V, E)$, if $V > 1$, there must be at least
$2$ leaves.

\subsubsection{Equivalent statements to the tree definition}

For a graph $T = (V, E)$, we have that the following are
equivalent: \begin{itemize}
  \item $T$ is a tree
  \item $T$ is connected and has no cycles
  \item $|E| = n - 1$ and T is connected
  \item $|E| = n - 1$ and T has no cycles
  \item T has a unique path between any two vertices
\end{itemize}

\subsection{Bipartitions}

\subsubsection{Definition of a bipartite graph}

For $G = (V, E)$, we have that $G$ is bipartite if there exists
$A \subset V$, $B \subset V$ such that $A$ and $B$ are disjoint
and the induced subgraphs of $A$ and $B$ have no edges. $A$
and $B$ are bipartitions of $G$.
\\[\baselineskip]
Saying $G$ is bipartite is equivalent to saying $G$ has no
cycles of odd length.
 
\subsubsection{Definition of a matching}

A matching in a graph is a set of disjoint edges.
\\[\baselineskip]
A matching is \textbf{perfect} if each vertex is contained in
some matching edge.

\subsubsection{Definition of a semi-matching}

For $k$ in $\mathbb{Z}_{>0}$, a $k$ to 1 semi-matching in a
bipartite graph $G$ with a bipartition $\{A, B\}$ is a subgraph
of $G$ where each vertex in $A$ has degree at most $k$ and
each vertex in $B$ has degree at most $1$.

\subsubsection{Definition of an augmenting path}

Given a matching $M$ in a bipartite graph $G = (V, E)$, 
an augmenting path is a set of vertices in $V$ connected
by edges $e_i$ in $E$ such that: \begin{align*}
  e_i \text{ is } \begin{cases}
    \text{in } M & \text{for } i \text{ odd} \\
    \text{not in } M & \text{for } i \text{ even}. \\
  \end{cases}
\end{align*} With the condition that the first and last vertices
in the path are not in the matching.

\subsubsection{Hall's Theorem}

For a bipartite graph $G = (V, E)$ with the bipartition $(A, B)$ has
a perfect matching if and only if $|A| = |B|$ and for all 
$X \subseteq A$, $|N(X)| \geq |X|$. 

\vfill

\section{Working on Graphs}

\subsection{Data Representations of Graphs}

\subsubsection{Adjacency matrix}

We have for a graph $G = (V, E)$, the adjacency matrix is a
$|V|$ by $|V|$ matrix $A = (a_{ij})$ where: \begin{gather*}
  a_{ij} = \begin{cases}
    1 & \text{if there's an edge from vertex $i$ to $j$} \\
    0 & \text{otherwise} \\
  \end{cases}
\end{gather*}

\subsubsection{Adjacency list}

We can represent a graph also by an array of linked lists or 
hash tables where
each element in the array represents a vertex and the corresponding
list represents the vertices in the neighbourhood of the vertex.

\newpage

\subsubsection{Comparision of representations}

We can compare some basic properties of the representations:
\begin{center}
  \renewcommand{\arraystretch}{1.4}
  \begin{tabular}{ | r || c | c | c |}
    \hline
    & Matrix & Linked Lists & Hash Tables \\ 
    \hline\hline
    Space & $\Theta(|V|^2)$ & $\Theta(|V| + |E|)$ & $\Theta(|V| + |E|)$ \\
    Finding an edge from $u$ & $O(1)$ & $O(\text{deg}(u))$ & $O(1)$ \\
    Finding the neighbourhood of $u$ & $O(|V|)$ & $O(\text{deg}(u))$ & $O(\text{deg}(u))$ \\
    \hline
  \end{tabular}
\end{center} This raises the question, why don't we always use
hash tables? Due to the probability of collisions in hash tables,
we opt for the linked list as it's more reliable for large graphs
(additionally, we are almost always are looking for a neighbourhood
not a specific edge).

\subsection{Search}

Generally with a graph searching algorithm, we have a data structure 
which is left undefined here (besides the fact we can add vertices to 
it and take vertices out). Starting with a vertex \texttt{u}, naming 
our data structure \texttt{data}, we perform the following:
\begin{lstlisting}
data Search(u) {
  add u to data;
  while (data is non-empty) {
    take x from data;
    if (x is not marked) {
      mark x;
      for (each edge (x, y)) {
        put y in data;
      }
    }
  }
}
\end{lstlisting} We have that this process always terminates, visits
every vertex in connected graphs, and has time complexity $O(|E|)$
(assuming the data operations are $O(1)$) where $E$ is the edge set.

\subsubsection{Breadth-first search}

If our data structure is a queue, we get breadth-first searching.
This causes vertices to be marked in distance order from the starting
point.
\paragraph{Shortest paths} By tracking distances, we can find shortest
paths using this searching style.
This is $O(|V| + |E|)$ in a graph $G = (V, E)$.

\subsubsection{Depth-first search}

If our data structure is a stack, we get depth-first searching.
This causes vertices to be marked the further they are from
the starting vertex.

\subsection{Djikstra's Algorithm}

For a weighted (non-negatively), directed graph (stored as a
adjacency list) we have that Djikstra's algorithm (given a)
starting vertex returns the fastest path to all other vertices 
(or a particular one if required). It is structured as follows:
\begin{lstlisting}
distances Djikstra(s) {
  let pq be our priority queue;
  let dist be our array of distances;
  for (each v) {
    dist[v] = infinity;
  }
  dist[s] = 0;
  for (each v) {
    pq->insert(v, dist(v));
  }
  while (pq is non-empty) {
    u = pq->extractMin();
    for (each edge (u, v)) {
      if (dist[v] > dist[u] + weight(u, v)) {
        dist[v] = dist[u] + weight(u, v);
        pq->decreaseKey(v, dist(v));
      }
    }
  }
  return dist;
}
\end{lstlisting} We have that the time complexity of the algorithm
varies across queues: \begin{center}
  \begin{tabular} {| r || c |}
    \hline
    & Runtime \\
    \hline \hline
    Linked List & $O(|V|^2 + |V||E|)$ \\
    \hline
    Binary Heap & $O((|V| + |E|)\log{(|V|)})$ \\
    \hline
    Fibonacci Heap & $O(|E| + |V|\log{(|V|)})$ \\
    \hline
  \end{tabular}
\end{center}
and has $O(|V| + |E|)$ space complexity across all queues.

\subsection{Minimum Spanning Trees}

\subsubsection{Definition of a spanning tree}

In a connected, undirected graph $G = (V, E)$, we have that a
spanning tree $T = (V', E')$ of $G$ is a subgraph of $G$ where
$T$ is a tree and $V = V'$.
\\[\baselineskip]
A spanning tree on $G$ is minimal if there is no other spanning tree
on $G$ with a lower weight.

\subsubsection{Kruskal's Algorithm}

For a weighted, connected, and undirected graph $G = (V, E)$, we have 
the following steps to the algorithm: \begin{enumerate}
  \item Generate a graph $T = (V, \emptyset)$
  \item Generate a disjoint set data structure $X$ of size $|V|$
  \item For each $v$ in $V$, perform \texttt{makeSet(v)} (where
  each vertex is defined by some unique integer in $\{1, \ldots, |V|\}$)
  \item Sort the edges by weight
  \item For each edge $(u, v)$ (in increasing order): \begin{itemize}
    \item If \texttt{findSet}$(u) \neq$ \texttt{findSet}$(v)$,
    perform \texttt{union}$(u, v)$ and add $(u, v)$ to $T$
  \end{itemize}
\end{enumerate} Overall, this runs in $O(|E| \log_2(|V|))$ time.

\vfill

\section{Fast Fourier Transforms}

\subsection{Polynomials}

\subsubsection{Definition of a Polynomial}

A polynomial of degree $n$ in $\mathbb{Z}_{\geq 0}$ is a function $A$: 
\begin{gather*}
  A(x) = \sum_{i = 0}^n a_ix^i,
\end{gather*} where $a_i$ are the coefficients of $A$. We say for 
$k > n$, $k$ is a degree-bound of $A$. We can represent this by listing
the coefficients, called the \textbf{coefficient representation}.

\newpage

\subsubsection{Fast Polynomial Evaluation}

We can evaluate polynomials quickly using \textit{Horner's Rule},
for a polynomial $A$ degree $n$: \begin{gather*}
  A(x) = a_0 + x(a_1 + x(a_2 + \cdots + x(a_n)))).
\end{gather*} This can be simplified in the following code:
\begin{lstlisting}
int polynomial(coeffs, x) {
  output = 0;
  for (i = n; i >= 0; i--) {
    output = (output * x) + coeffs[i];
  }
  return output;
}
\end{lstlisting} We have that this is $O(n)$.

\subsubsection{Point Intersection with Polynomials}

For a given set of points of size $n$, we have that there exists a 
unique polynomial with degree-bound $n$ such that the polynomial
intersects all the given points.

\subsubsection{Point-Value Representation}

We can represent a polynomial by a set of points it intersects like so:
\begin{gather*}
  \{(x_0, y_0), \ldots, (x_n, y_n)\},
\end{gather*} for a polynomial degree $n + 1$.

\subsubsection{Polynomial Addition}

For two polynomials $A, B$ with coefficients $a_i, b_i$ and degrees $n, m$ 
respectively, we have that: \begin{gather*}
  (A + B)(x) =  \sum_{i = 0}^{\text{max}(n, m)}(a_i + b_i)x^i.
\end{gather*} If $m > n$ or vice versa, we pad out the shorter
polynomial with zeroes. We can do this with the point-value representation
by adding the '$y$-values'. We have that addition as it's defined here
is $O(n)$.

\subsubsection{Polynomial Multiplication}

For two polynomials $A, B$ with coefficients $a_i, b_i$ and degrees $n, m$ 
respectively, we have that: \begin{gather*}
  C(x) = (A \cdot B)(x) = \sum_{i = 0}^{k} c_ix_i,
\end{gather*} where $k = 2 \cdot \text{max}(n, m)$ and: \begin{gather*}
  c_i = \sum_{j = 0}^ia_jb_{j - 1}.
\end{gather*} We can do this with the point-value representation,
for: \begin{gather*}
  A = \{(x_0, y_0), \ldots, (x_n, y_n)\}, \\
  B = \{(x_{n + 1}, z_0), \ldots, (x_{n + m}, z_m)\},
\end{gather*} We have that: \begin{gather*}
  C = A \cdot B = \{(x_0, y_0 \cdot z_0), \ldots, (x_k, y_k \cdot z_k)\}
\end{gather*} This is much easier, yielding
an $O(n)$ algorithm rather than an $O(n^2)$ algorithm.

\subsection{Fast Fourier Transform}

\subsubsection{Roots of Unity}

The idea is that we evaluate a polynomial to perform pointwise
multiplication and then interpolate back into a polynomial.
We need to evaluate a polynomial of degree $n$ at $n + 1$ points to
convert it to point-value form. We use the $n + 1$ roots of unity:
\begin{gather*}
  \omega_{n+1}^k = e^{\frac{2\pi i}{n + 1}k},
\end{gather*} for $k$ in $\{0, 1, \ldots n\}$. Therefore considering:
\begin{gather*}
  y_k = A(\omega_{n + 1}^k),
\end{gather*} for $A$ a polynomial, $k$ as above, and the vector of 
all ordered $y_k$ being the \textbf{Discrete Fourier Transform (DFT)} 
of the coefficient vector of $A$.

\paragraph{Cancellation Lemma:} we have that 
$\omega_{dn}^{dk} = \omega_{n}^{k}$.

\paragraph{Halving Lemma:} we have that if $n$ is even, the set of all
the squared roots of unity is just the set of roots of unity for $\frac{n}{2}$.
\\[\baselineskip]
\textit{This is true due to the Cancellation Lemma, we have:} \begin{gather*}
  (\omega_{2k}^j)^2 = \omega_{2k}^{2j} = \omega_{k}^j.
\end{gather*}

\subsubsection{Method of the Fast Fourier Transform}

For a polynomial $A$ degree $n$, 
we define $A^{[0]}$ and $A^{[1]}$ as: \begin{align*}
  A^{[0]} &= a_0 + a_2x + \cdots + a_{n - 2}x^{(n / 2) - 1} \\
  A^{[1]} &= a_1 + a_3x + \cdots + a_{n - 1}x^{(n / 2) - 1},
\end{align*} so we have that: \begin{gather*}
  A(x) = A^{[0]}(x^2) + xA^{[1]}(x^2).
\end{gather*} So, we can split a DFT computation into two
equally sized parts, compute them, and then combine them in linear time.

\subsection{Polynomial Multiplication}

So, the steps are laid out, for polynomials $A, B$ with degree bound 
$n$, as follows: \begin{itemize}
  \item Set the degree of $A$ and $B$ to $2n$, padding with zeroes
  \item Perform the fast Fourier transform
  \item Form our point-value representation and multiply pointwise
  \item Interpolate with the inverse fast Fourier transform.
\end{itemize} This process is $O(n \log(n))$.

\vfill

\section{Dynamic Programming}

Dynamic programming is the process of solving programming problems
by breaking them down into \textbf{overlapping} subproblems,
computing the base cases and storing the solutions to be later
composed into a solution.

\newpage

\subsection{Largest Empty Square}

This problem is about finding the largest square in a $n \times n$
black and white image such that the square does not contain a black
pixel.

\subsubsection{A recursive algorithm}

To find the largest square at the position $(x, y)$ 
(bottom-right corner at $(x, y)$), we use the
following algorithm: \begin{lstlisting}
size LargestSquare(x, y) {
  if ((x, y) is black) return 0;
  if ((x == 1) or (y == 1)) return 1;
  return min(
    LargestSquare(x - 1, y - 1),
    LargestSquare(x - 1, y),
    LargestSquare(x, y - 1));
}
\end{lstlisting} The time complexity of this algorithm, however,
is exponential.
\\[\baselineskip]
We get this as each cell barring the first and last columns and rows 
have cells where \texttt{LargestSquare} is computed three times 
(as they  are checked from below, to the right, and below and to 
the right).

\subsubsection{Storing the solutions to subproblems}

We now consider storing our solutions to cells so we do not repeat
ourselves, take \texttt{A} to be an $n \times n$ array
where each cell is undefined as first: \begin{lstlisting}
  size LargestSquare_Stored(x, y) {
    if ((x, y) is black) A[x, y] = 0;
    if ((x == 1) or (y == 1)) A[x, y] = 1;
    if (A[x, y] is undefined) A[x, y] = min(
      LargestSquare_Stored(x - 1, y - 1),
      LargestSquare_Stored(x - 1, y),
      LargestSquare_Stored(x, y - 1));
    return A[x, y];
  }
\end{lstlisting} This can be adapted to work iteratively from
$(1, 1)$ down to $(x, y)$ with a complexity of $O(n^2)$.

\newpage

\subsection{Weighted Interval Scheduling}

We have a set of $n$ intervals, a triple containing a
start time $s_i$, finishing time $f_i$, and a weight $w_i$.
A schedule is a set of intervals such that they do not overlap
(with respect to their starting and finishing times).
\\[\baselineskip]
The intervals are provided as an array $A$, sorted ascending by
finishing times.

\subsubsection{The rightmost compatible interval $p$}

We define $p$ as a function from $\textbf{Interval} \to \textbf{Interval}$,
which takes an interval $i$ and returns the \textbf{latest} interval that 
finishes \textbf{before} $i$.
\\[\baselineskip]
This can be precomputed beforehand in $O(n\log_2(n))$ time by using 
binary search ($O(\log_2(n))$).

\subsubsection{A recursive algorithm}

For $n$ intervals indexed by $\{1, \ldots, n\}$ in $A$: \begin{lstlisting}
weight WIS(i) {
  if (i == 0) return 0;
  return max(WIS(i - 1), WIS(p(i)) + w_i);
}
\end{lstlisting} However, this leads to WIS(i) being calculated more than once
for some $i$.

\subsubsection{Storing the solutions to subproblems}

Now, we consider a global array of schedules $S$ where
$S[i]$ contains \texttt{WIS}($i$) or is undefined: \begin{lstlisting}
weight WIS_Stored(n) {
  if (n == 0) return 0;
  for (int i = 1; i <= n; i++) {
    S[i] = max(S[i - 1], S[p(i)] + w_i);
  }
  return S[n];
}
\end{lstlisting} This takes $O(n)$ time.

\newpage

\subsubsection{Returning the schedule}

We can find the schedule using our
stored $S$ from the previous section: \begin{lstlisting}
schedule FindSchedule(i) {
  if (i == 0) return [];
  if (S(i - 1) <= S(p(i)) + w_i) {
    return FindSchedule(p(i)) ++ [i]; 
  }
  return FindSchedule(i - 1);
}
\end{lstlisting} This takes $O(n)$ time.

\subsection{Self-balancing Trees}

\paragraph{Perfect balance} a tree where each path from the root to a leaf has the
same length is perfectly balanced.
\\[\baselineskip]
Consider a tree where each node can have between $2$ and $4$ (inclusive) children 
(where a child can be empty) called a $2-3-4$ tree. Take note of the following: 

  \paragraph{2-node} a node with value $v$, $2$ children, and $1$ key
  where the left child is less than or equal to $v$ and 
  the right child is greater than or equal to $v$.
  \paragraph{3-node} a node with values $v_1, v_2$, $3$ children, and $2$ keys
  where the left child is less than or equal to $v_1$, 
  the middle child is between $v_1$ and $v_2$ (inclusive),
  and the right child is greater than or equal to $v_2$.
  \paragraph{4-node} a node with values $v_1, v_2, v_3$, $4$ children, and $3$ keys
  where the left child is less than or equal to $v_1$, 
  the left-middle child is between $v_1$ and $v_2$ (inclusive),
  the right-middle child is between $v_2$ and $v_3$ (inclusive),
  and the right child is greater than or equal to $v_3$.

\subsubsection{The height of $2-3-4$ trees}

If we suppose all the nodes in the tree are $2$/$4$ nodes we get the
worst/best case heights for a $2-3-4$ tree with $n$ elements: \begin{center}
  \begin{tabular}{| c | c |}
    \hline
    Node Type & Height \\
    \hline 
    \hline 
    2 & $O(\log_2(n))$ \\
    \hline 
    4 & $O(\log_4(n))$ \\
    \hline
  \end{tabular}
\end{center}

\subsubsection{Insertion on $2-3-4$ trees}

\paragraph{Splitting} this operation works on a $4$-node. The middle value of the node
is added to the parent and two 2-nodes are formed from the remains.
\\[\baselineskip]
When inserting an element $k$ we search for where the element belongs whilst
splitting any $4$-nodes into $2$-nodes as we recurse. We convert the bottom node from
type $t$ to $t + 1$ ($t \neq 3$ by our algorithm structure) and insert our value.

\subsubsection{Deletion on $2-3-4$ trees}

\paragraph{Fusion} this operation works on two $2$-nodes with a shared parent. A relevant key
is taken from the parent and used to form a $4$-node. Fusing the root decreases the height of 
tree and is the only operation with this property.

\paragraph{Transferring} this operation works on a $2$-node and a $3$-node with a shared parent.
A key from the parent is added to the 2-node whilst a key from the 3-node is added to the parent
\\[\baselineskip]
We will consider the cases when deleting a value $k$. For leaves, we search for the value,
transferring and fusing to convert 2-nodes on the path, we delete the value, converting the
node from a node type $t$ to a type $t - 1$ ($t \neq 2$ by our algorithm structure). For
non-leaves, we delete the predecessor of $k$, $k'$ (always a leaf) and replace $k$ with $k'$.

\subsection{Search}

\subsubsection{Binary Search Trees}

For an element $k$ in a binary search tree (acting as our dynamic search structure), 
the left child of an element is less than or equal to $k$ and the right child is 
greater than or equal to $k$.
\\[\baselineskip]
However, this results in $O(\log_2(n))$ time for \texttt{insert}, \texttt{find}, 
and \texttt{delete} for balanced trees but becomes $O(n)$ for unbalanced trees.

\end{document} 