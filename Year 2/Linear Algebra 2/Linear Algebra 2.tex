\documentclass[a4paper, 12pt, twoside]{article}
\usepackage[left = 3cm, right = 3cm]{geometry}
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
\usepackage{mathtools}
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{multicol}

\begin{document}

\title{Linear Algebra 2 Notes}
\date{}
\author{\textit{paraphrased by} Tyler Wright}
\maketitle

\vfill

\textit{An important note, these notes are absolutely \textbf{NOT}
  guaranteed to be correct, representative of the course, or rigorous.
  Any result of this is not the author's fault.}

\newpage

\section{Groups, Rings, and Fields}

\subsection{Definition of a Group}

A group is a set $G$ combined with a group operation 
$\circ : G \times G \to G$ such that: \begin{itemize}
  \item For all $g, h, j$ in $G$, $g(hj) = (gh)j$ (associativity)
  \item There exists $e$ in $G$ such that $eg = ge = g$ for all
  $g$ in $G$
  \item For all $g$ in $G$, there exists $g^{-1}$ in $G$ such that
  $gg^{-1} = g^{-1}g = e$ where $e$ is the identity of $G$.
\end{itemize}

\subsection{Definition of a Homomorphism}

A homomorphism between two groups $G, H$ is a function $f : G \to H$
such that $f(gh) = f(g)f(h)$ for all $g, h$ in $G$.

\subsection{Properties of Homomorphisms}

We can derive some properties of homomorphisms, for
$G, H$ groups, and $f : G \to H$ a homomorphism: \begin{itemize}
  \item The image of the identity in $G$ is the identity in $H$
  \item The kernel of $f$ is a subgroup of $G$
  \item The image of $f$ is a subgroup of $H$
  \item Bijective homomorphisms are isomorphisms.
\end{itemize}

\subsection{Definition of a Ring}

A ring with unity is a set $R$ along with an addition map $+$, and
a multiplication map $\circ$ where $+, \circ : R \times R \to R$
such that: \begin{itemize}
  \item $(R, +)$ is an abelian group (of which the identity is called zero)
  \item The multiplication operation is associative
  \item The multiplication operation has a two-sided identity not equal
  to the zero identity (called one)
  \item For all $a, b, c$ in $R$, $a(b+c) = ab + ac$ and $(a+b)c = ac + bc$.
\end{itemize} A ring is commutative if the multiplication operation is commutative.

\subsection{Definition of a Subring}

For the ring $R = (R', +, \circ)$ and $S$ a set, $S$ is a subring
of $R$ if $S \subseteq R'$ and $(S, +, \circ)$ is a ring.

\subsection{Definition of a Ring Homomorphism}

For rings with unity $R$ and $S$, $f : R \to S$ is a ring homomorphism if for all
$a, b$ in $R$: \begin{align*}
  f(a + b) &= f(a) + f(b) \\
  f(ab) &= f(a)f(b) \\
  f(1_R) &= 1_S
\end{align*} \textit{Essentially, this says that $f$ is a homomorphism for the
groups formed by $R$ and $S$ under addition and multiplication.}

\subsection{Definition of a Field}

A field $\mathbb{F}$ is a ring with unity with the following properties: \begin{itemize}
  \item $(\mathbb{F}\setminus\{0\}, \circ)$ is an abelian group.
\end{itemize}

\subsection{Definition of the Field Characteristic}

For a field $\mathbb{F}$, the field characteristic  char($\mathbb{F}$) is the 
smallest positive integer $n$ such that: \begin{gather*}
  \sum_{i = 1}^{n} 1 = 1 + 1 + \ldots + 1 = 0,
\end{gather*} or zero if no such value $n$ exists.

\subsection{Definition of the Algebraic Closure of Fields}

A field $\mathbb{F}$ is called algebraically closed if all non-constant polynomials 
with coefficients in $\mathbb{F}$ also has a root in $\mathbb{F}$.

\section{Vector Spaces}

\subsection{Definition of a Vector Space}

A vector space over a field $\mathbb{F}$ is a set $V$ with an addition
operation $+ : V \times V \to V$ and a scalar multiplication operations
$\circ : \mathbb{F} \times V \to V$ such that for all $a, b$ in $\mathbb{F}$
and $v, w$ in $V$: \begin{itemize}
  \item $(V, +)$ is an abelian group
  \item $1 \circ v = v$ where $1$ is the multiplicative identity of $\mathbb{F}$
  \item $(ab) \circ v = a \circ (b \circ v)$
  \item $(a + b) \circ v = a \circ v + b \circ v$
  \item $a \circ (v + w) = a \circ v + a \circ w$.
\end{itemize}

\subsection{Definition of a Subspace}

For $V$ a vector space over the field $\mathbb{F}$ and $W$ a set, $W$ is a 
subspace of $V$ if it is a subset of $V$ and is a vector space with respect 
to the addition and scalar multiplication defined by $V$.
\\[\baselineskip]
It is sufficient to verify that for any $a$ in $\mathbb{F}$ and $v, w$ in $W$ we
have that $a(v + w)$ is in $W$.

\subsection{Definition of a Linear Combination}

For a set $V$ with addition operation $+$, a field $\mathbb{F}$ and $n$ in 
$\mathbb{N}$, a linear combination of $v_1, \ldots, v_n$ in $V$ is: \begin{gather*}
  \sum_{i = 1}^n a_iv_i,
\end{gather*} for $a_1, \ldots, a_n$ in $\mathbb{F}$.

\subsection{Definition of the Span}

For a set $V$ with addition operation $+$ and a field $\mathbb{F}$, the span 
of $W \subseteq V$ is the set of all the linear combinations of the values
in $W$. Denoted by span($W$).

\subsection{Definition of Linear Independence}

For a vector space $V$ and $W \subseteq V$, we say $W$ is linearly dependent if
there exists a non-trivial linear combination of all the vectors in $W$ 
equal to zero (and linearly independent otherwise).

\subsection{Properties of Linear Independence}

For a vector space $V$ with $W \subseteq V$: \begin{itemize}
  \item $0 \in W \Rightarrow W$ is linearly independent
  \item $W$ linearly independent $\Rightarrow$ any $X \subseteq W$ 
  is linearly independent
  \item If there's a linearly dependent subset of $W$, then $W$
  is linearly dependent.
\end{itemize}

\subsection{Definition of a Basis}

For a vector space $V$ with $W \subseteq V$, if $W$ is linearly independent
and span($W$) = $V$, we say that $W$ is a basis of $V$.
\\[\baselineskip]
Saying $W$ is a basis is equivalent to saying that each vector in $V$ can
be \textbf{uniquely} written as a linear combination of vectors in $W$.
\\[\baselineskip]
Additionally, for finite vector spaces, we have that all bases have the same
amount of elements.

\subsection{Definition of Dimension}

For non-infinite bases, we say that the value of the basis is the dimension
of the vector space it is a member of. Vector spaces with such bases are called
finite-dimensional and all other vector spaces are infinite-dimensional.
\\[\baselineskip]
By convention, for a vector space $V$, dim($\{0_V\}$) $= 0$.

\newpage

\subsection{Isomorphisms from Dimension}

For $V, W$ finite-dimensional vector spaces over $\mathbb{F}$
with dim$(V)$ = dim$(W)$, then $V \cong W$.
\\[\baselineskip]
If we set $n = \text{dim}(V)$, we have that $V \cong \mathbb{F}^{n}$.
\\[\baselineskip]
\textit{Such an isomorphism can be found by mapping a vector in
terms of some chosen basis vectors 
($v = a_1v_1 + a_2v_2 + \cdots + a_nv_n$) to the coefficients
$(a_1, a_2, \ldots, a_n)$.}

\section{Linear Maps}

\subsection{Definition of a Linear Map}

Let $V, W$ be vector spaces over a field $\mathbb{F}$, we have that
$f:V \to W$ is a linear map if for all $a, b$ in $\mathbb{F}$ and
$u, v$ in $V$: \begin{gather*}
  f(au+bv) = af(u) + bf(v).
\end{gather*} A bijective linear map is called an isomorphism.
If $f: V \to W$ is an isomorphism, we say that $V$ and $W$ are 
isomorphic, denoted by $V \cong W$. 

\subsection{The Kernel of Linear Maps}

Let $V, W$ be vector spaces over a field $\mathbb{F}$, and
$f : V \to W$ be a linear map. We define the kernel of $f$ as: \begin{gather*}
  \text{Ker}(f) = \{v \in V : f(v) = 0_{\mathbb{F}}\}.
\end{gather*} Saying Ker($f)$ is $\{0_\mathbb{F}\}$ is equivalent
to saying $f$ is injective.

\subsection{The Image of Linear Maps}

Let $V, W$ be vector spaces over a field $\mathbb{F}$, and
$f : V \to W$ be a linear map. We define the image of $f$ as: \begin{gather*}
  \text{Im}(f) = \{w \in W : \exists \, v \in V \text{ with } f(v) = w\}.
\end{gather*} Saying Im($f)$ is $W$ is equivalent
to saying $f$ is surjective.

\subsection{The Inverse of Linear Maps}

For a bijective linear map $f$, the inverse of $f$ is also linear.

\subsection{Properties of the Set of Linear Maps}

For $V, W$ vector spaces over a field $\mathbb{F}$, we define
$\mathcal{L}(V, W)$ to be the set of all linear maps from $V$
to $W$.

\subsection{The Rank-Nullity Theorem}

For $V, W$ finite-dimensional vector spaces and 
$f : V \to W$ a linear map, we have that: \begin{gather*}
  \text{dim}(V) = \text{dim(Ker(}f)) + \text{dim(Im(}f)).
\end{gather*} Thus, for a linear map $f : V \to V$, if $f$ is
injective or surjective then it's an isomorphism.

\section{Matrices}

\subsection{Definition of a Matrix}

For $m, n$ in $\mathbb{Z}_{>0}$ and $\mathbb{F}$ a field. An
$m \times n$ matrix with entries in $\mathbb{F}$ is a map
$M : [m] \times [n] \to \mathbb{F}$, more commonly written as 
$M = (a_{ij})$ representing the rectangular array of values held
by $M$.
\\[\baselineskip]
The set of all $m \times n$ matrices over $\mathbb{F}$ is denoted by 
$M_{m \times n}(\mathbb{F})$.

\subsection{Types of Matrix}

For $m, n$ in $\mathbb{Z}_{>0}$ and $\mathbb{F}$ a field, let $M$
be in $M_{m \times n}(\mathbb{F})$. We have the following types of
matrix: \begin{itemize}
  \item \textbf{Square}: where $m = n$
  \item \textbf{Upper Triangular}: if $a_{ij} = 0$ for $i > j$
  \item \textbf{Lower Triangular}: if $a_{ij} = 0$ for $i < j$
  \item \textbf{Diagonal}: if $a_{ij} = 0$ for $i \neq j$
  \item \textbf{Symmetric}: if $a_{ij} = a_{ji}$
  \item \textbf{Anti-symmetric}: if $a_{ij} = -a_{ji}$.
\end{itemize}

\subsection{Properties of the Space of Matrices}

For $m, n$ in $\mathbb{Z}_{>0}$ and $\mathbb{F}$ a field, we have
that $M_{m \times n}(\mathbb{F})$ is a vector space over $\mathbb{F}$
where matrices are added and multiplied by scalars component-wise.
So, for \newline $M_1 = (a_{ij}), M_2 = (b_{ij})$ in $M_{m\times n}$
and $c$ in $\mathbb{F}$ we have:
\begin{align*}
  cM_1 &= (ca_{ij}) \\
  M_1 + M_2 &= (a_{ij} + b_{ij}).
\end{align*} Additionally, the zero vector is $M_0 = (0)$ and the vector
space has a basis consisting of $M_{ij}$ where all entries are zero
except the $(i, j)^{\text{th}}$ entry. This leads to the conclusion
that the dimension is $mn$ and thus that 
$M_{m \times n} \cong \mathbb{F}^{mn}$.

\subsection{Matrix Multiplication}

For $a, b, c$ in $\mathbb{Z}_{>0}$ and a field $\mathbb{F}$, 
we can define the multiplication of the two matrices $X = (x_{ij})$ in
$M_{a \times b}$ and $Y = (y_{ij})$ in $M_{b \times c}$ as follows: 
\begin{gather*}
  XY = (\sum_{k = 1}^b x_{ik}y_{kj}).
\end{gather*} This operation is not commutative in general but is
associative.
\\[\baselineskip]
For $A, B$ in $M_n$, we have that $AB$ is also in $M_n$. This,
along with matrix addition, makes $M_n$ a ring with unity with
multiplicative identity $I_n = (\delta_{ij})$. However, there exists
$A, B$ in $M_n$ such that $AB=0$ so, $M_n$ is not a field.

\subsection{Matrices of Linear Maps}

For $V, W$ vector spaces over a field $\mathbb{F}$, for some
$m, n$ in $\mathbb{Z}_{>0}$ we have $A = \{v_1, \ldots, v_n\}$,
$B = \{w_1, \ldots, w_n\}$ bases for $V$ and $W$ respectively.
Given $f$ in $\mathcal{L}(V, W)$, the matrix associated to $f$
(with respect to the bases $A$ and $B$) is the $m \times n$ matrix:
\begin{gather*}
  M_{BA}(f) = (a_{ij}),
\end{gather*} where we define $a_{ij}$ by: \begin{gather*}
  f(v_j) = \sum_{i = 1}^m a_{ij}w_i,
\end{gather*} for each $j$ in $[n]$.

\subsection{Matrices of Composed Linear Maps}

For $U, V, W$ vector spaces over a field $\mathbb{F}$, for some
$l, m, n$ in $\mathbb{Z}_{>0}$ we have $A = \{u_1, \ldots, u_n\}$,
$B = \{v_1, \ldots, v_n\}$, $C = \{w_1, \ldots, w_n\}$ bases for 
$U, V, W$ respectively. Given $g, f$ in $\mathcal{L}(V, W)$, we have:
\begin{gather*}
  M_{CA}(g \circ f) = M_{CB}(g)M_{BA}(f).
\end{gather*}

\subsection{Transition Matrices}

For a finite-dimensional vector space $V$, with an identity $I$ 
and bases $A, A'$, we call $M_{A'A}(I) = C_{A'A}$ a transition
matrix.
\\[\baselineskip]
We have that $C_{A'A}$ is invertible and $C_{A'A}^{-1} = C_{AA'}$.
\\[\baselineskip]
\textit{Essentially, the transition matrix transforms between bases.}

\subsection{Matrix Transitions}

For a finite-dimensional vector space $V$, with $f:V \to V$ a linear
operator, and bases $A, B$: \begin{align*}
  M_{BB}(f) &= C_{AB}^{-1}M_{AA}(f)C_{AB} \\
  &= C_{BA}M_{AA}(f)C_{AB}.
\end{align*}

\subsection{Similar Matrices}

For matrices $A', A$, we say that $A'$ and $A$ are similar if there
exists an invertible matrix $C$ such that: \begin{gather*}
  A' = C^{-1}AC.
\end{gather*} This is denoted by $A' \sim A$. Similarity forms
an equivalence relation on the space of square matrices.
\\[\baselineskip]
If we have $A \sim A'$ and $A$ represents some linear operator $f$
for some basis $B$, then we have that for some basis $B'$, $f$ has
matrix $A'$.

\section{Eigenvectors and Eigenvalues}

\subsection{Definition of an Eigenvectors and Eigenvalues}

For a vector space $V$ over $\mathbb{F}$ with $f: V \to V$ a 
linear operator, a non-zero vector $v$ in $V$ is an eigenvector
if $f(v) = \lambda v$ for some $\lambda$ in $\mathbb{F}$ which is
called the eigenvalue corresponding to $v$.

\subsection{Definition of an Eigenspace}

For a vector space $V$ over $\mathbb{F}$ with $f: V \to V$ a 
linear operator and some eigenvalue $\lambda$, we define the
eigenspace of $\lambda$ as the set of eigenvectors with eigenvalue
$\lambda$.
\\[\baselineskip]
This is denoted by $E(\lambda)$ and $E(\lambda)\cup\{0_{V}\}$ forms
a subspace of $V$. The dimension of $E(\lambda)$ is the geometric
multiplicity of $\lambda$.

\section{Direct Sums and Projections}

\subsection{Definition of a Direct Sum}

For $V, W$ vector spaces, we define the direct product of $V$ and
$W$ as: \begin{gather*}
  V \oplus W = \{(v, w) : v \in V, w \in W\},
\end{gather*} with addition and scalar multiplication defined
coordinate-wise and zero vector $(0_V, 0_W)$.

\subsection{The Equivalence of Direct Sums}
For $V, W \subseteq U$, we have that the following are equivalent:
\begin{itemize}
  \item $U = V \oplus W$
  \item Each element in $U$ can be written uniquely as the sum
  of elements in $V$ and $W$
  \item The map $f : V \oplus W \to U; (v, w) \mapsto v + w$ is
  isomorphism.
\end{itemize}

\newpage

\subsection{The Addition Map for Direct Sums}

For $V, W$ subspaces of a vector space $U$, and $f : V \oplus W \to U$
defined by: \begin{gather*}
  f((v, w)) = v + w,
\end{gather*} we have that: \begin{itemize}
  \item $f$ is linear
  \item $f$ is injective if and only if $V \cap W = \{0\}$
  \item $f$ is surjective if and only if $V \cup W$ spans $U$.
\end{itemize}

\subsection{Projections}

For $V, W$ subspaces of $U$ with $U = V \oplus W$, the projection
\textbf{onto} $V$ along $W$ is the linear operator $P_{V, W} : U \to U$
where: \begin{gather*}
  P_{V, W}(u) = v,
\end{gather*} where $u = v + w$ for some unique $v$ in $V$ and
$w$ in $W$.
\\[\baselineskip]
We have that for a linear operator $P$, $P$ is a projection if
and only if $P \circ P = P$.

\subsection{$f$-invariance}

For a vector space $V$ with $U \subseteq V$ a subspace and 
$f : U \to U$ a linear operator, we have that $U$ is $f$-invariant
if for all $u$ in $U$ we have $f(u)$ in $U$.
\\[\baselineskip]
The eigenspaces of $f$ are examples of $f$-invariant spaces.

\subsection{Matrices of Linear Maps (using $f$-invariance)}

For $U, W \subseteq V$ subspaces of the vector space $V$ such that
$V = U \oplus W$, let $B_U, B_W$ be finite bases of $U$ and $V$
respectively. If we have a linear operator $f : V \to V$ such that
$U$ and $W$ are $f$-invariant, we have that the matrix with respect
to the basis $B = B_U \cup B_W$ of $f$ has the following block form:
\begin{gather*}
  M_{BB}(f) = \begin{pmatrix}
    M_{B_VB_V}(f) & 0 \\
    0 & M_{B_WB_W}(f)
  \end{pmatrix}.
\end{gather*}
 
\section{Quotient Spaces}

\subsection{Definition of a Quotient Space}

For a vector space $V$ with $W \subseteq V$ a subspace. We define an equivalence
relation on $V$ by declaring: \begin{gather*}
  v_1 \sim v_2 \text{ if } v_1 - v_2 \in W.
\end{gather*} The set of equivalence classes is called the quotient of $V$
by $W$ and is denoted by $V/W$. For some $v$ in $V$, we denote the class containing
$v$ by $v + W$ (similarly to cosets in Introduction to Group Theory). So, we have:
\begin{gather*}
  V/W = \{v + W : v \in V\},
\end{gather*} with addition and multiplication defined for 
$v_1, v_2$ in $V$ and $a$ in the field: \begin{align*}
  (v_1 + W) + (v_2 + W) &= (v_1 + v_2) + W \\
  a(v_1 + W) &= av_1 + W.
\end{align*}

\subsection{Linear Map to the Quotient Space}

For a vector space $V$ with $W \subseteq V$ a subspace, we can
define $\pi : V \to V/W$ for some $v$ in $V$ by $f(v) = v + W$.
We have that $\pi$ is linear and its kernel is $W$.

\subsection{Isomorphisms formed by Linear Maps}

For $V, W$ vector spaces and $f : V \to W$ a linear map, we have
an isomorphism $\text{Im}(f) \cong V/\text{Ker}(f)$.

\subsection{Existence of a Linear Operator on the Quotient Space}

For a vector space $V$ with $W \subseteq V$ a subspace and a linear
operator $f : V \to V$, there exists a well-defined operator
$\bar{f} : V/W \to V/W$ if and only if $W$ if $f$-invariant.
We call this the induced operator on $V/W$.

\newpage

\subsection{Matrices formed using Quotient Spaces}

Consider a finite-dimension vector space $V$ and $f : V \to V$ a linear
operator with $W$ an $f$-invariant subspace of $V$. 
If we have $B_W$ a basis for $W$, that we extend to a basis $B$ of 
$V$ and set $A$: \begin{gather*}
  A = \{v + W : v \in B \setminus B_W \},
\end{gather*} a basis of $V/W$ and we can form
a matrix in block form: \begin{gather*}
  M_{BB}(f) = \begin{pmatrix}
    M_{B_WB_W}(f) & * \\
    0 & M_{AA}(\bar{f})
  \end{pmatrix},
\end{gather*} where $\bar{f}$ is the induced operator on $V/W$ and $*$
marks the area of the matrix which we cannot determine.

\section{Dual Spaces}

\subsection{Definition of a Dual Space}

For $V$ a vector space over $\mathbb{F}$, we have that the dual space
$V^*$ is $\mathcal{L}(V, \mathbb{F})$, the set of linear maps from 
$V$ to $\mathbb{F}$. We have that addition and scalar multiplication
are defined for some $v$ in $V$, $f, g$ in $V^*$, and $a$ in 
$\mathbb{F}$: \begin{align*}
  (f + g)(v) &= f(v) + g(v), \\
  (af)(v) &= af(v).
\end{align*}

\subsection{Definition of a Dual Basis}

For $V$ a finite-dimensional vector space over $\mathbb{F}$, with 
dim$(v) = n$ and a basis $B = \{v_1, \ldots, v_n\}$. 
We define the dual basis $B^* = \{v_1^*, \ldots, v_n^*\}$
by defining $v_i^* : V \to \mathbb{F}$ as the unique linear map
such that: \begin{gather*}
  v_i^*(v_j) = \begin{cases}
    0 & \text{if } i \neq j \\
    1 & \text{if } i = j.
  \end{cases}
\end{gather*} 
\newpage
Equivalently, for $v$ in $V$, we have that there's 
unique $(a_1, \ldots, a_n)$ in $\mathbb{F}$ such that: \begin{gather*}
  v = \sum_{i = 1}^n a_iv_i,
\end{gather*} so we let $v_i$ be such that: \begin{gather*}
  v_i^*(v) = v_i^*\left(\sum_{j = 1}^n a_jv_j\right) 
  = \sum_{j = 1}^n a_jv_i^*(v_j).
\end{gather*} We have that $B^*$ is a basis for $V^*$. Additionally,
we have that $V$ and $V^*$ are isomorphic by the isomorphism mapping
$v_i$ to $v_i^*$.

\subsection{Definition of the Annihilator}

For $V$ a vector space over $\mathbb{F}$ with $S \subseteq V$,
the annihilator of $S$ is the subspace $S^0$ of $V^*$ where
for $f$ in $S^0$, $S \subseteq \text{Ker}(f)$ (or rather, 
for all $s$ in $S$, $f(s) = 0$).

\subsection{Properties of the Annihilator}

For $V$ a vector space with $U, W \subseteq V$
subspaces, we have that: \begin{itemize}
  \item $(U + W)^0 = U^0 \cap W^0$
  \item $U \subseteq W \Rightarrow W^0 \subseteq U^0,$
\end{itemize} and for $V$ finite-dimensional, \begin{itemize}
  \item $(U \cap W)^0 = W^0 + U^0$
  \item $\text{dim}(W) + \text{dim}(W^0) = \text{dim}(V).$
\end{itemize}

\subsection{Isomorphism to the Double Dual Space}

For $V$ a finite-dimensional vector space over $\mathbb{F}$, 
we have $F: V \to V^{**}$. That is: \begin{gather*}
  V^{**} = \mathcal{L}(V^*, \mathbb{F}) = \mathcal{L}(\mathcal{L}(V, \mathbb{F}), \mathbb{F}),
\end{gather*} so for some $v$ in $V$ we have: 
\begin{gather*}
  F(v) : V^* \to \mathbb{F}.
\end{gather*} We define $F$ for some $f$ in $V^*$ as follows: 
\begin{gather*}
  F(v)(f) = f(v).
\end{gather*} We have that $F$ is an isomorphism.

\subsection{Definition of the Transpose}

For $V, W$ vector spaces with $f : V \to W$ a linear map. We define
the transpose as $f^t : W^* \to V^*$ where for $g$ in $W^*$, $v$ in
$V$: \begin{gather*}
  f^t(g) = (g \circ f).
\end{gather*} So, for some $v$ in $V$: \begin{gather*}
  f^t(g)(v) = (g \circ f)(v) = g(f(v)).
\end{gather*}

\subsection{The Transpose and Matrices}

If we have $V, W$ finite-dimensional vector spaces over $\mathbb{F}$ with bases 
$A = \{v_1, \ldots, v_n\}$, $B = \{w_1, \ldots, w_m\}$ and corresponding
dual bases $A^* = \{v_1^*, \ldots, v_n^*\}$, $B^* = \{w_1^*, \ldots, w_m^*\}$
respectively, we have that for some linear map $f : V \to W$, and
$f^t : W^* \to V^*$ the transpose map: \begin{gather*}
  M_{BA}(f) = \left( M_{A^*B^*}(f^t) \right)^t.
\end{gather*} \textit{That is, for a given map, the matrix of 
transpose map is itself the matrix transpose of the matrix of the map.}

\section{Rank and Determinants}

\subsection{Elementary Row Operations}

For a field $\mathbb{F}$, take $A$ in $M_{m,n}(\mathbb{F})$. The elementary
row operations are: \begin{itemize}
  \item Swapping
  \item Multiplying by scalars in $\mathbb{F}\setminus\{0_\mathbb{F}\}$
  \item Adding a multiple of a row to another
\end{itemize}

\subsection{Elementary Matrices}

The $n \times n$ elementary matrices are: \begin{itemize}
  \item $E_1(i, j)$ : obtained by swapping the $i^{th}$ and $j^{th}$ rows
  of the identity
  \item $E_2(c, i)$ : obtained by scaling the $i^{th}$ row of the identity
  by $c$ non-zero
  \item $E_3(c, i, j)$ : obtained by adding $c$ times row $i$ to row $j$
  where $i \neq j$.
\end{itemize} We have that any elementary row operation can be realised as
left-multiplication by a corresponding elementary matrix. As a consequence
of the definition, we have that elementary matrices are invertible and
have elementary inverses.

\subsection{Echelon Form}

A matrix $A$ is in echelon form if each row has the form: \begin{gather*}
  (0, \ldots, 0, 1, *, \ldots, *),
\end{gather*} where each row has more leading zeroes than the one above and
the first row has any amount of leading zeroes. Every matrix can be put in
this form via Gaussian elimination.

\subsection{Decomposition via Elementary Matrices}

For an $n \times n$ matrix $A$, there exists elementary matrices
$E_1, \ldots, E_k$ such that $E_1\cdots E_kA = B$ where:
\begin{gather*}
  B = \begin{cases}
    \text{the identity} & \text{if $A$ is invertible} \\
    \text{a matrix with a final row consisting of all zeroes} & \text{otherwise.}
  \end{cases}
\end{gather*}

\subsection{Rank}

For $A = (a_{ij})$ a matrix in $M_{m,n}(\mathbb{F})$, we denote its rows by
$A_{(1)}, \ldots, A_{(m)}$ and columns by $A^{(1)}, \ldots, A^{(n)}$. We
say: \begin{itemize}
  \item The row rank of $A$ is the dimension of the subspace of spanned by 
  $A_{(1)}^t, \ldots, A_{(m)}^t$ in $\mathbb{F}^m$
  \item The column rank of $A$ is the dimension of the subspace of spanned by 
  \newline $A^{(1)}, \ldots, A^{(n)}$ in $\mathbb{F}^n$.
\end{itemize} We have these are equal, so can generally refer to the rank 
of a matrix.
\\[\baselineskip]
If $E_1, \ldots, E_k$ are elementary matrices, we have that the rank of $A$
is equal to the rank of $E_1\cdots E_kA$. Similarly, similar matrices have
the same rank.

\subsection{Rank of Matrices from Linear Maps}

For $A$ an $m \times n$ matrix on $\mathbb{F}$, we can define a map 
$f : \mathbb{F}^n \to \mathbb{F}^m$ by $v \mapsto Av$. We have that
the rank of $A$ is the dimension of the image of $f$.
Thus, invertible $n \times n$ matrices have rank $n$.

\subsection{Permutations}

\subsubsection{Definition of a Permutation}

Let $[n]$ be $\{1, 2, \ldots, n\}$. A permutation of $[n]$ is a bijection
$\sigma : [n] \to [n]$. We define the set of all permutations on $[n]$ as
$S_n$.

\subsubsection{Decomposition of Permutations}

All permutations can be written as a product of disjoint cycles. Thus,
all permutations can be written as a product of transpositions.

\end{document}