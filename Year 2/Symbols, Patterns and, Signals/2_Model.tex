\section{Data Modelling}

A model is a description of data, it should generalise, either as 
an abstraction or a simplification.We can quantify the performance 
of a model by how well it maps the data to the desired solution.

\subsection{Model Parameters}

Models are defined in terms of parameters which could be obtained
through trial and error or training data (through tuning or
training the model).

\subsection{Overfitting}

Training a model too hard on a specific data set can cause it
to 'overfit'. This means it performs very well on the trained
data set but does not generalise well.
\\[\baselineskip]
This can happen in a variety of cases, some including: \begin{itemize}
    \item There is too little data,
    \item There is too little data representing some
        key part of the data distribution,
    \item The function class is too complex.
\end{itemize}

\subsubsection{Cross-validation}

We can decide to train our data on only a fraction of our data set
and then use the remainder to test whether our data is overfitted.
This can be repeated multiple times with different subsets of the
original data set. However, this can fail on smaller data sets or
ones with many parameters.
\\[\baselineskip]
We can combine cross-validation with regularisation to produce
regularising matrices to penalise overfitted parameters.

\subsection{Bayesian Fitting}

Suppose we are interested in some weighting vector $\mathbf{w}$ of 
a distribution. We consider the prior $\mathbb{P}(\mathbf{w})$
and posterior $\mathbb{P}(\mathbf{w} \, | \, \textbf{X})$ probability
of our vector given some sample data $\mathbf{X}, \mathbf{y}$ and 
relate them using Bayes' theorem: \begin{gather*}
    \mathbb{P}(\mathbf{w} \, | \, \mathbf{X})
    = \frac{
        \mathbb{P}(\mathbf{X} \, | \, \mathbf{w})\mathbb{P}(\mathbf{w})
    }{\mathbb{P}(\mathbf{X})}.
\end{gather*} By taking the natural logarithm, we see that:
\begin{gather*}
    \ln(\mathbb{P}(\mathbf{w} \, | \, \mathbf{X}))
    = \ln(\mathbb{P}(\mathbf{X} \, | \, \mathbf{w}))
    + \ln(\mathbb{P}(\mathbf{w}))
    + \ln\left(\frac{1}{\mathbb{P}(\mathbf{X})}\right).
\end{gather*} We consider a fixed data set, so $\mathbf{X}$ is not
a variable. Assuming we are sampling from the multivariate normal,
we can calculate 
$\ln(\mathbb{P}(\mathbf{w} \, | \, \mathbf{X}))$ in terms of
the equation above and in terms of the multivariate normal
distribution. Equating these allows us to identify equations
for the regularised, linear regression mean and variance parameters:
\begin{align*}
    \hat\mu &= ((\mathbf{X}^{\text{T}}\mathbf{X})^{-1}
    + \sigma^2 \text{Var}(\mathbf{w})^{-1})^{-1}\mathbf{X}\mathbf{y}, \\
    \hat\Sigma &= \sigma^2 ((\mathbf{X}^{\text{T}}\mathbf{X})^{-1}
    + \sigma^2 \text{Var}(\mathbf{w})^{-1})^{-1}.
\end{align*} 

\subsection{Deterministic Models}

Deterministic models produce an output without a measure of
confidence in that output.

\subsubsection{Single Variable Linear Regression}

For a two dimensional set of data: \begin{gather*}
    D = \{(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\}.
\end{gather*} We calculate a gradient $r$ and an intercept $c$ to 
uniquely define the regression line $(y = rx + c)$ on $D$: \begin{align*}
    r &= \frac{
        \sum_{i = 1}^n (x_i \cdot y_i) - n\bar{x}\bar{y}
    }{
        \sum_{i = 1}^n (x_i^2) - n\bar{x}^2
    }, \\
    c &= \bar{y} - r \cdot \bar{x}.
\end{align*} Outliers have disproportionate effects due to the
squares used by the measure. 
\\[\baselineskip]
For a two dimensional set of data: 
\begin{gather*}
    D = \{(x_1, y_1), (x_2, y_2), \ldots, (x_n, y_n)\}.
\end{gather*} where each $x_i$ is $k$-dimensional,
we can use matrices to calculate our coefficients: \begin{gather*}
    R = (X^{\text{T}}X)^{-1}X^{\text{T}}Y,
\end{gather*} where: \begin{gather*}
    R = \begin{pmatrix}
        r_0 \\ r_1 \\ \vdots \\ r_k
    \end{pmatrix} 
    \qquad
    Y = \begin{pmatrix}
        y_1 \\ y_2 \\ \vdots \\ y_n
    \end{pmatrix}
    \qquad
    X = \begin{pmatrix}
        1      & \leftarrow & x_1    & \rightarrow \\
        1      & \leftarrow & x_2    & \rightarrow \\
        \vdots &            & \vdots &             \\
        1      & \leftarrow & x_n    & \rightarrow
    \end{pmatrix}.
\end{gather*} 
Instead of using $k$-dimensional data, we can instead use
powers of each $x^i$ to form fitted polynomials.
In this case, we have a least squares linear regression line
$y = r_0 + r_1x^1 + \cdots + r_nx^n$.

\subsubsection{Multivariate Linear Regression}

We consider a set of data with $k$ data points, with
the $i^{\text{th}}$ data point corresponding to 
$(\mathbf{x}_i,\mathbf{y}_i)$ the $n$ and $m$-dimensional,
input and output vectors (resp.). We assume each $\mathbf{y}_i$
is multivariate normal, conditional on its corresponding 
$\mathbf{x}_i$ with some $n$-dimensional weight vector 
$\mathbf{w}$: \begin{gather*}
    \mathbb{P}(\mathbf{y}_i \, | \, \mathbf{x}_i, \mathbf{w})
    = f(\mathbf{y}_i),
\end{gather*} where $f$ is the multivariate normal probability
density function with mean $\mathbf{x}_i \cdot \mathbf{w}$ and
variance $\sigma^2$. We can expand this to the full data set 
with: \begin{gather*}
    \mathbf{y} = \begin{pmatrix}
        \leftarrow & \mathbf{y}_1 & \rightarrow \\
        \leftarrow & \mathbf{y}_2 & \rightarrow \\
                   & \vdots &          \\
        \leftarrow & \mathbf{y}_n & \rightarrow
    \end{pmatrix}
    \qquad
    \mathbf{X} = \begin{pmatrix}
        \leftarrow & \mathbf{x}_1 & \rightarrow \\
        \leftarrow & \mathbf{x}_2 & \rightarrow \\
                   & \vdots &                   \\
        \leftarrow & \mathbf{x}_n & \rightarrow
    \end{pmatrix}.
\end{gather*} 
giving us: \begin{gather} \label{multivarlinreg}
    \mathbb{P}(\mathbf{y} \, | \, \mathbf{X}, \mathbf{w}) 
    = f(\mathbf{y}),
\end{gather} where $f$ is the multivariate normal probability
density function with mean $\mathbf{X} \cdot \mathbf{w}$ and
variance $\sigma^2 I$. By using maximum likelihood estimation,
we can see that our optimal weight vector $\mathbf{\hat{w}}$
is equal to $(\mathbf{X}^{\text{T}}\mathbf{X})^{-1}
\mathbf{X}^{\text{T}}\mathbf{y}$.

\newpage

\subsubsection{Regularisation and Multivariate Linear Regression}

In an effort to penalise exceptionally large and potentially
overfitted regression weights, we use regularisation.
We modify the prediction of the distribution of $\mathbf{y}$
seen in (\ref{multivarlinreg}): \begin{gather*}
    \mathbb{P}(\mathbf{y} \, | \, \mathbf{X}, \mathbf{w}) 
    = f(\mathbf{y})
    \left[
        e^{-\frac{1}{2}\mathbf{w}^\text{T} \Lambda \mathbf{w}}
    \right],
\end{gather*} with diagonal regularisation matrix $\Lambda$ 
so when we take the natural logarithm: \begin{gather*}
    \ln(\mathbb{P}(\mathbf{y} \, | \, \mathbf{X}, \mathbf{w})) 
    = \ln(f(\mathbf{y}))
        - \frac{1}{2}\mathbf{w}^\text{T} \Lambda \mathbf{w}.
\end{gather*} Note that where $\Lambda = \text{diag}(\lambda_1,
\ldots, \lambda_n)$ and $\mathbf{w} = (w_1, \ldots, w_n)$: 
\begin{gather*}
    \mathbf{w}^\text{T} \Lambda \mathbf{w} 
    = \sum_{i = 1}^n \lambda_i \cdot w_i^2.
\end{gather*} We have the maximum likelihood estimate in this case
is: \begin{gather*}
    \mathbf{\hat{w}} = ((\mathbf{X}^{\text{T}}\mathbf{X})^{-1}
    + \sigma^2 \Lambda)^{-1}\mathbf{X}\mathbf{y}.
\end{gather*}

\subsection{Probabilistic Models}

Probabilistic models pair outputs with measures of confidence,
probability.

\subsubsection{Maximum Likelihood Estimation}

We first define the argmax function:
\begin{gather*}
    \text{argmax}_\theta(f(\theta)),
\end{gather*} for some objective function $f$, is the $\theta$
such that $f(\theta)$ is maximised - denoted by $\hat\theta$.
We define argmin similarly, and can see that:
\begin{align*}
    \text{argmax}_\theta(f(\theta)) 
    &= \text{argmax}_\theta(\ln(f(\theta))) \\
    &= \text{argmin}_\theta(-\ln(f(\theta))),
\end{align*} as $\ln$ is monotone increasing.
\\[\baselineskip]
When considering some data set $D$ with parameters
encapsulated by $\theta$, we take $\mathbb{P}(D \, | \, \theta)$ 
to be the function returning the probability of $D$ occuring based 
on the parameters of $\theta$. Thus: \begin{gather*}
    \text{argmax}_\theta(\mathbb{P}(D \, | \, \theta)),
\end{gather*} represents the parameter values that
are most likely to produce this data set. Practically,
this can be carried out by taking the derivative of
$\ln(\mathbb{P}(D \, | \, \theta))$ and taking derivatives 
of it with respect to the parameters. We set the derivatives
to zero and solve for $\theta$.

\subsubsection{Approximate Bayesian Computation}

If we have a very small amount of data, we can take simulations
of the (suspected) underlying distributions varying across its
parameters and identify the simulations which contain our small
data set. We can plot the histogram of these supersets to see
which parameters are most likely to produce our small data set.
\\[\baselineskip]
However, in practice - this doesn't work very well.

\subsubsection{Maximum a Posteriori}

If we have some prior information about the distribution of
$\theta$, we can consider assimilating that into our estimation.
We take the posterior to be: \begin{gather*}
    \mathbb{P}(\theta \, | \, D) 
    = \frac{
        \mathbb{P}(D \, | \, \theta)\mathbb{P}(\theta)
        }{Z},
\end{gather*} for some normalising term $Z$. Note that,
$\mathbb{P}(D \, | \, \theta)$ is our likelihood and 
$\mathbb{P}(\theta)$ is our prior. Thus, we extend
our Maximum Likelihood Estimator to use the posterior instead,
giving us the Maximum a Posteriori estimator: \begin{gather*}
    \hat\theta = \text{argmax}_\theta(\mathbb{P}(\theta \, | \, D)).
\end{gather*}